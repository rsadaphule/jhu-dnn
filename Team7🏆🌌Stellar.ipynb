{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsadaphule/jhu-dnn/blob/main/Team7%F0%9F%8F%86%F0%9F%8C%8CStellar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<small><font color=gray>Notebook author: <a href=\"https://www.linkedin.com/in/olegmelnikov/\" target=\"_blank\">Oleg Melnikov</a> ©2021 onwards</font></small><hr style=\"margin:0;background-color:silver\">\n",
        "\n",
        "**[<font size=6>🌌Stellar</font>](https://www.kaggle.com/competitions/5jun23jh-stellar/rules)**. [**Instructions**](https://colab.research.google.com/drive/1riOGrE_Fv-yfIbM5V4pgJx4DWcd92cZr#scrollTo=ITaPDPIQEgXV) for running Colabs."
      ],
      "metadata": {
        "id": "q3pqxgX4DxeH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<small>**(Optional) CONSENT.** <mark>[ X ]</mark> We consent to sharing our Colab (after the assignment ends) with other students/instructors for educational purposes. We understand that sharing is optional and this decision will not affect our grade in any way. <font color=gray><i>(If ok with sharing your Colab for educational purposes, leave \"X\" in the check box.)</i></font></small>"
      ],
      "metadata": {
        "id": "_dTfQBUZyNQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive; drive.mount('/content/drive')   # OK to enable, if your kaggle.json is stored in Google Drive"
      ],
      "metadata": {
        "id": "qrogZ_8bD9tZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca254c3-db35-4846-defb-75c53681b06d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8XoC8VqBXGs",
        "outputId": "d9ebbb1a-235e-483e-f48d-b6b4a4d24490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "- competition is now set to: 5jun23jh-stellar\n"
          ]
        }
      ],
      "source": [
        "!pip -q install --upgrade --force-reinstall --no-deps kaggle > log  # upgrade kaggle package (to avoid a warning)\n",
        "!mkdir -p ~/.kaggle                                           # .kaggle folder must contain kaggle.json for kaggle executable to properly authenticate you to Kaggle.com\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json >log  # First, download kaggle.json from kaggle.com (in Account page) and place it in the root of mounted Google Drive\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json > log                   # Alternative location of kaggle.json (without a connection to Google Drive)\n",
        "!chmod 600 ~/.kaggle/kaggle.json                              # give only the owner full read/write access to kaggle.json\n",
        "!kaggle config set -n competition -v 5jun23jh-stellar        # set the competition context for the next few kaggle API calls. !kaggle config view - shows current settings\n",
        "!kaggle competitions download >> log                          # download competition dataset as a zip file\n",
        "!unzip -o *.zip >> log                                        # Kaggle dataset is copied as a single file and needs to be unzipped.\n",
        "lb =!kaggle competitions leaderboard --show                   # print public leaderboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "%%capture\n",
        "%reset -f\n",
        "from IPython.core.interactiveshell import InteractiveShell as IS; IS.ast_node_interactivity = \"all\"\n",
        "import numpy as np, pandas as pd, time, matplotlib.pyplot as plt, seaborn as sns, os, tqdm, re, sys, cv2, skimage\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LogisticRegression as LR\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA, LinearDiscriminantAnalysis as LDA\n",
        "ToCSV = lambda df, fname: df.round(2).to_csv(f'{fname}.csv', index_label='id') # rounds values to 2 decimals\n",
        "\n",
        "class Timer():\n",
        "  def __init__(self, lim:'RunTimeLimit'=60): self.t0, self.lim, _ = time.time(), lim, print(f'⏳ started. You have {lim} sec. Good luck!')\n",
        "  def ShowTime(self):\n",
        "    msg = f'Runtime is {time.time()-self.t0:.0f} sec'\n",
        "    print(f'\\033[91m\\033[1m' + msg + f' > {self.lim} sec limit!!!\\033[0m' if (time.time()-self.t0-1) > self.lim else msg)\n",
        "\n",
        "np.set_printoptions(linewidth=100, precision=2, edgeitems=2, suppress=True)\n",
        "pd.set_option('display.max_columns', 20, 'display.precision', 2, 'display.max_rows', 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CyC-JlZFga1",
        "outputId": "cded07a1-87d4-49d7-c845-e5242fe463b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.32 s, sys: 165 ms, total: 1.48 s\n",
            "Wall time: 1.81 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('XY_Stellar.csv'); df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "X00bQLb5FpxU",
        "outputId": "691e2809-7a22-4017-db4a-efb318a4a50d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         alpha  delta      u      g      r      i      z  run_ID  cam_col  \\\n",
              "0        11.64  21.28  26.28  26.15  24.05  18.87  19.00    8848        5   \n",
              "1       173.09  42.21  22.51  22.83  22.21  19.55  19.96    4156        3   \n",
              "...        ...    ...    ...    ...    ...    ...    ...     ...      ...   \n",
              "199998  131.31  44.27  24.07  24.64  21.63  19.20  19.03    7076        3   \n",
              "199999   22.59   0.25  25.30  25.56  24.09  19.41  19.96    5164        4   \n",
              "\n",
              "        field_ID  redshift  plate    MJD  fiber_ID Class  \n",
              "0            272      0.84   7740  56824       833   NaN  \n",
              "1            486      0.81   9041  58067       428   NaN  \n",
              "...          ...       ...    ...    ...       ...   ...  \n",
              "199998       251      0.55   6014  56166      1021     G  \n",
              "199999       511      1.26   9590  57969       878     G  \n",
              "\n",
              "[200000 rows x 15 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-46eb7de6-5eb8-4737-b750-77a6c4ff8a6f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alpha</th>\n",
              "      <th>delta</th>\n",
              "      <th>u</th>\n",
              "      <th>g</th>\n",
              "      <th>r</th>\n",
              "      <th>i</th>\n",
              "      <th>z</th>\n",
              "      <th>run_ID</th>\n",
              "      <th>cam_col</th>\n",
              "      <th>field_ID</th>\n",
              "      <th>redshift</th>\n",
              "      <th>plate</th>\n",
              "      <th>MJD</th>\n",
              "      <th>fiber_ID</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11.64</td>\n",
              "      <td>21.28</td>\n",
              "      <td>26.28</td>\n",
              "      <td>26.15</td>\n",
              "      <td>24.05</td>\n",
              "      <td>18.87</td>\n",
              "      <td>19.00</td>\n",
              "      <td>8848</td>\n",
              "      <td>5</td>\n",
              "      <td>272</td>\n",
              "      <td>0.84</td>\n",
              "      <td>7740</td>\n",
              "      <td>56824</td>\n",
              "      <td>833</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>173.09</td>\n",
              "      <td>42.21</td>\n",
              "      <td>22.51</td>\n",
              "      <td>22.83</td>\n",
              "      <td>22.21</td>\n",
              "      <td>19.55</td>\n",
              "      <td>19.96</td>\n",
              "      <td>4156</td>\n",
              "      <td>3</td>\n",
              "      <td>486</td>\n",
              "      <td>0.81</td>\n",
              "      <td>9041</td>\n",
              "      <td>58067</td>\n",
              "      <td>428</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199998</th>\n",
              "      <td>131.31</td>\n",
              "      <td>44.27</td>\n",
              "      <td>24.07</td>\n",
              "      <td>24.64</td>\n",
              "      <td>21.63</td>\n",
              "      <td>19.20</td>\n",
              "      <td>19.03</td>\n",
              "      <td>7076</td>\n",
              "      <td>3</td>\n",
              "      <td>251</td>\n",
              "      <td>0.55</td>\n",
              "      <td>6014</td>\n",
              "      <td>56166</td>\n",
              "      <td>1021</td>\n",
              "      <td>G</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199999</th>\n",
              "      <td>22.59</td>\n",
              "      <td>0.25</td>\n",
              "      <td>25.30</td>\n",
              "      <td>25.56</td>\n",
              "      <td>24.09</td>\n",
              "      <td>19.41</td>\n",
              "      <td>19.96</td>\n",
              "      <td>5164</td>\n",
              "      <td>4</td>\n",
              "      <td>511</td>\n",
              "      <td>1.26</td>\n",
              "      <td>9590</td>\n",
              "      <td>57969</td>\n",
              "      <td>878</td>\n",
              "      <td>G</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200000 rows × 15 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-46eb7de6-5eb8-4737-b750-77a6c4ff8a6f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-46eb7de6-5eb8-4737-b750-77a6c4ff8a6f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-46eb7de6-5eb8-4737-b750-77a6c4ff8a6f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()   # observe datatypes and any missing values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFp0IV3BJR9_",
        "outputId": "757cd031-69cb-438a-caa6-ce275f3a7314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 200000 entries, 0 to 199999\n",
            "Data columns (total 15 columns):\n",
            " #   Column    Non-Null Count   Dtype  \n",
            "---  ------    --------------   -----  \n",
            " 0   alpha     200000 non-null  float64\n",
            " 1   delta     200000 non-null  float64\n",
            " 2   u         200000 non-null  float64\n",
            " 3   g         200000 non-null  float64\n",
            " 4   r         200000 non-null  float64\n",
            " 5   i         200000 non-null  float64\n",
            " 6   z         200000 non-null  float64\n",
            " 7   run_ID    200000 non-null  int64  \n",
            " 8   cam_col   200000 non-null  int64  \n",
            " 9   field_ID  200000 non-null  int64  \n",
            " 10  redshift  200000 non-null  float64\n",
            " 11  plate     200000 non-null  int64  \n",
            " 12  MJD       200000 non-null  int64  \n",
            " 13  fiber_ID  200000 non-null  int64  \n",
            " 14  Class     160000 non-null  object \n",
            "dtypes: float64(8), int64(6), object(1)\n",
            "memory usage: 22.9+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change string labels to numbers in order of increasing size of the entity (Star < Quasi Star < Galaxy)\n",
        "# df.Class = df.Class.apply(lambda C: -1 if C=='S' else 0 if C=='Q' else 1 if C=='G' else None)"
      ],
      "metadata": {
        "id": "UDA4wJqELlOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vX = df.query('Class!=Class').drop('Class', axis=1)  # slice a test sample\n",
        "tXY = df.query('Class==Class')                       # slice training sample\n",
        "tX, tY = tXY.drop('Class', axis=1), tXY.Class        # split into training I/O"
      ],
      "metadata": {
        "id": "f7OuVizOFsFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ScatterCorrHist(df):\n",
        "  def corrdot(*args, **kwargs):\n",
        "    # credit: https://stackoverflow.com/questions/48139899\n",
        "    corr_r = args[0].corr(args[1], 'pearson')\n",
        "    corr_text = f\"{corr_r:2.2f}\".replace(\"0.\", \".\")\n",
        "    ax = plt.gca();\n",
        "    ax.set_axis_off();\n",
        "    msz = abs(corr_r) * 5000   # marker size\n",
        "    fsz = abs(corr_r) * 40 + 5 # font size\n",
        "    ax.scatter([.5], [.5], msz, [corr_r], alpha=0.5, cmap='coolwarm', vmin=-1, vmax=1, transform=ax.transAxes)\n",
        "    ax.annotate(corr_text, [.5, .5,],  xycoords=\"axes fraction\", ha='center', va='center', fontsize=fsz)\n",
        "\n",
        "  sns.set(style='white', font_scale=.8);\n",
        "  g = sns.PairGrid(df, aspect=1, diag_sharey=False);\n",
        "  g.fig.set_size_inches(20,10)\n",
        "  g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color':'red'}, scatter_kws={'s':1});\n",
        "  g.map_diag(sns.histplot, kde_kws={'color':'black'});\n",
        "  g.map_upper(corrdot);\n",
        "  g.fig.suptitle(\"Scatter plot, Correlations and histograms on diagonal\", y=1);\n",
        "  _ = plt.subplots_adjust(hspace=0.02, wspace=0.02);\n",
        "  _ = plt.show();\n",
        "\n",
        "# ScatterCorrHist(tXY.head(200))"
      ],
      "metadata": {
        "id": "i4gelET6Hb2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmr = Timer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4_C58bbHuja",
        "outputId": "158eff54-ea68-4614-a6e7-344697274b35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳ started. You have 60 sec. Good luck!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr color=red>\n",
        "\n",
        "<font size=5>⏳</font> <strong><font color=orange size=5>Your Code, Documentation, Ideas and Timer - All Start Here...</font></strong>\n",
        "\n",
        "**Student's Section** (between ⏳ symbols): add your code and documentation here."
      ],
      "metadata": {
        "id": "3NcTKbw3KhAn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpyLNt3god0c"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "## **Task 1. Preprocessing Pipeline**\n",
        "\n",
        "**Explain elements of your preprocessing pipeline i.e. feature engineering, subsampling, clustering, dimensionality reduction, etc.**\n",
        "\n",
        "- Our pre-processing pipeline relied heavily on feature engineering. The objective was to get a good mix of existing features and generated features to increase the score.\n",
        "- Transformers were used to scale and transform existing features into new features.\n",
        "\n",
        "**1. Why did you choose these elements? (Something in EDA, prior experience,...? Btw, EDA is not required)**\n",
        "\n",
        "Many of the attributes were identifiers (IDS); MJD is a *Modified Julian Date*; these values were removed: ['run_ID', 'cam_col', 'field_ID', 'MJD', 'fiber_ID'].\n",
        "\n",
        "The target Variables are skewed. There are many more G *galaxies* than S *stars* or Q *Quasars combined.  It was necessary to keep track of how the preprocessing impacted the distribtion so as to not remove too many from one of the lower target counts.\n",
        "\n",
        "Many of these features were found using brute force search. A collection of candidate operations was assembled and a baseline performance was computed on the unmodified features. Random operations were then chosen from that collection and applied to random features. The performance was then recomputed.\n",
        "\n",
        "If performance improved, then that operation-feature pair was reported. This process was then repeated. We then sorted through these suggested features by hand and picked those that seemed simple and likely to be applicable.\n",
        "\n",
        "One issue with a technique like this is its liability to overfit the training data if one blindly applies all of the suggested features. Performance on the training set will grow arbitrarily high as the number of features grows.\n",
        "\n",
        "For this reason, we applied a number of safeguards against overfitting.\n",
        "- Firstly, cross-validation was used and only improvements on the validation sets were reported.\n",
        "- Secondly, the data was only processed in this way in 20,000 row chunks. When a very promising feature was reported, that feature was then tested against a different 20,000 row chunk of training data to confirm that the same increase in performance was observed.\n",
        "\n",
        "Many times, testing against a different chunk did not improve and when this happened, the suggested engineered feature was thrown away. Only when substantial evidence was collected that an automatically generated feature would improve the performance of our model was that engineered feature finally adopted.\n",
        "\n",
        "**2. How do you evaluate the effectiveness of these elements?**\n",
        "- All components that could used a random seed, all used the same random seed. This was set at the top of the project. When *colabs* were exchanged between teammates, the scores would be identical as a result.\n",
        "- As new features were generated, the performance was measured using five fold cross validation.\n",
        "- Different models to include *logistic regression*, *SGDClassifier*, *QuadraticDiscriminantAnalysis*,  and *LinearDiscriminantAnalysis* were evaluated with LSA being selected as the model to use.\n",
        "\n",
        "\n",
        "**3. What else have you tried that worked or didn't?**\n",
        "- After deciding on *LDA* many combinations of variables were trialed that would inch the score up ever so slightly.\n",
        "- As discussed earler a collection of Lambda functions were created to try different combinations. This was very helpful.\n",
        "- MinMaxScaling worked well on a selction of features.\n",
        "- A transform called *QuantileTransformer* worked well and increased the score.  Features were added based on scaling attributes using it.\n",
        "- StandardScaler and RobustScaller did not work as well as MinMaxScaler.\n",
        "- VarianceThreshold was used and did improve the processing speed significantly.\n",
        "- Third order polynomials were tried but with so many features, the processing time exceeded the threshold permitted.\n",
        "- Second order polynomials with many features worked well.\n",
        "- Feature selection using *, SelectKBest*, worked well when trialing third order polynomials. With the second order polynomials, this was set to keep all the features. The pipeline was tried with and without it; with it the performance was much better than without it.\n",
        "- Many combinations of features were trialed. All possibilities cannot be mentioned here but when the score increased the new feature was kept.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGJRwzqHob4o"
      },
      "source": [
        "## **Task 2. Modeling Approach**\n",
        "Explain your modeling approach, i.e. ideas you tried and why you thought they would be helpful.\n",
        "\n",
        "1. How did these decisions guide you in modeling?\n",
        "1. How do you evaluate the effectiveness of these elements?\n",
        "1. What else have you tried that worked or didn't?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi6ZjgtWnb58"
      },
      "source": [
        "**Explain your modeling approach**\n",
        "Our modeling approch started after reviewing the data.\n",
        "1. Feature engineering -- generate pleanty of features.\n",
        "2. Eavaluate models and select the one that balanced performance measured in terms of the score and speed in terms of getting it done within the acceptable timeframe. Logistic Regression, SGDClassifier, QuadraticDiscriminantAnalysis, and LinearDiscriminantAnalysis (LDA) were all evaluated.\n",
        "3. LDA selected and team worked on a generating and trialing different feature combinations to increase the score.\n",
        "\n",
        "**1. How did these decisions guide you in modeling?**\n",
        "The Score and getting it done in time were what drove feature and model selection.  Getting a good combination of features was essential to completing on time.\n",
        "\n",
        "**2. How do you evaluate the effectiveness of these elements?**\n",
        "In trialing new things 5 fold cross validation was used in some cases. While in others only a single score was used.\n",
        "\n",
        "**3. What else have you tried that worked or didn't?**\n",
        "Many of the other models were tried but did not perform as well as LDA. Hyperparameter selection was performed on those and it proved very time consuming.  LDA was simpler and had few hyperparameters to adapt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJs0jS4fIO1j"
      },
      "source": [
        "**Below is a work of Team 7.**\n",
        "- Kristina Moralic\n",
        "- Thomas Marshall\n",
        "- Shawn Simon\n",
        "- Anthony Sheller\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "from copy import deepcopy\n",
        "from sklearn.pipeline import Pipeline,make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest,VarianceThreshold\n",
        "import warnings\n",
        "#from sklearn.exceptions import DataConversionWarning\n",
        "warnings.filterwarnings(action='ignore')\n",
        "from sklearn.preprocessing import QuantileTransformer, MinMaxScaler\n",
        "\n",
        "#import sklearn\n",
        "#sklearn.set_config(display=\"diagram\"),\n",
        "\n",
        "# Initalize Random State used by all that use a random state\n",
        "RANDOM_STATE = 5\n",
        "def shuffle_and_split_data(data, test_ratio,r_int=2):\n",
        "    '''\n",
        "    Shuffle and split accordingly.\n",
        "\n",
        "    The return value will be stratified as the data is imbalanced\n",
        "\n",
        "    '''\n",
        "    data = data.sample(frac=1,random_state = RANDOM_STATE).reset_index(drop=True)\n",
        "    train, test = train_test_split(data, test_size=test_ratio, stratify=df['Class'],random_state=RANDOM_STATE)\n",
        "    return train, test\n",
        "\n",
        "# A dictionary of useful lambda functions for creating new features\n",
        "ops2 = {\n",
        " 'log': lambda x: np.log(np.maximum(0, x) + 1),\n",
        " 'exp': lambda x: np.exp(x),\n",
        " 'sig': lambda x: 1 / (1 + np.exp(-x)),\n",
        " 'si+': lambda x: x + 1 / (1 + np.exp(-x)),\n",
        " 'mul': lambda x, y: x * y,\n",
        " 'mul3': lambda x, y, z: x * y * z,\n",
        " 'logadd': lambda x, y: np.log(np.maximum(0, x + y) + 1),\n",
        " 'expadd': lambda x, y: np.exp(x + y),\n",
        " 'norm': lambda x: (x - x.min()) / (x.max() - x.min()),\n",
        " 'zsc': lambda x: (x - x.median()) / (x.std()),\n",
        " 'div': lambda x, y: x / y,\n",
        " 'asin': lambda x: np.arcsin(x),\n",
        " 'acos': lambda x: np.arccos(x),\n",
        " 'atan': lambda x: np.arctan(x),\n",
        " 'tanh': lambda x: np.tanh(x),\n",
        " 'drop': None\n",
        " }\n",
        "\n",
        "# Drop the features not useful in our pipeline\n",
        "df = df.drop(['run_ID', 'cam_col', 'field_ID', 'MJD', 'fiber_ID'], axis=1)\n",
        "\n",
        "\n",
        "# Preprocessing -- create new features\n",
        "df['logred'] = ops2['log'](df.redshift)\n",
        "df['logalpha'] = ops2['log'](df.alpha)\n",
        "df['logi'] = ops2['log'](df.i)\n",
        "df['logz'] = ops2['log'](df.z)\n",
        "df['logg'] = ops2['log'](df.g)\n",
        "df['logr'] = ops2['log'](df.r)\n",
        "df['logplate'] = ops2['log'](df.plate)\n",
        "df['logred2'] = ops2['logadd'](df.redshift, df.logred)\n",
        "df['z_div_g'] = ops2['div'](df.z, df.g)\n",
        "df['ig'] = ops2['mul'](df.i, df.g)\n",
        "df['redbool'] = (df.redshift < 0.15).astype(float)\n",
        "df['redbool2'] = (df.redshift > 0.8).astype(float)\n",
        "df['atan.logg'] = ops2['atan'](df.logg)\n",
        "df['zbool'] = (df.z < 19.5).astype(float)\n",
        "df['gbool'] = (df.g < 23.1).astype(float)\n",
        "df['atan_alpha'] = ops2['atan'](df.alpha)\n",
        "\n",
        "# Using Scaler to create a new feature\n",
        "scaler = MinMaxScaler(feature_range=(0.00001, 1+0.00001))\n",
        "df['redshift_scaled']= np.log(scaler.fit_transform(np.array(df['redshift']).reshape(-1, 1)))\n",
        "\n",
        "# use the QuantileTrasformer which helped get a few more points.\n",
        "trans = QuantileTransformer(n_quantiles=100, output_distribution='normal',random_state=RANDOM_STATE)\n",
        "df['q_redshift'] = trans.fit_transform(np.array(df['redshift_scaled']).reshape(-1,1))\n",
        "df['q_g'] = trans.fit_transform(np.array(df['g']).reshape(-1,1))\n",
        "df['q_i'] = trans.fit_transform(np.array(df['i']).reshape(-1,1))\n",
        "df['q_r'] = trans.fit_transform(np.array(df['r']).reshape(-1,1))\n",
        "df['q_z'] = trans.fit_transform(np.array(df['z']).reshape(-1,1))\n",
        "df['q_plate'] = trans.fit_transform(np.array(df['plate']).reshape(-1,1))\n",
        "df['q_alpha'] = trans.fit_transform(np.array(df['alpha']).reshape(-1,1))\n",
        "df['q_delta'] = trans.fit_transform(np.array(df['delta']).reshape(-1,1))\n",
        "\n",
        "# Make a copy of the df, split it into the df with the 'Class' feature set\n",
        "# And the testset for submission to Kaggle\n",
        "df_orig= deepcopy(df)\n",
        "df = deepcopy(df_orig.query('Class==Class'))\n",
        "df_test = deepcopy(df_orig.query('Class != Class').drop('Class', axis=1))\n",
        "\n",
        "# Do the split. Note it's shuffled before the split is done.\n",
        "train,test = shuffle_and_split_data(df,0.2)\n",
        "\n",
        "# Create the train and the train_y (targets)\n",
        "train, train_y = train.drop('Class',axis=1),train.Class\n",
        "# Create the test and test_y\n",
        "test,test_y = test.drop('Class',axis=1),test.Class\n",
        "\n",
        "# Make the pipeline\n",
        "m = make_pipeline(PolynomialFeatures(interaction_only=True,degree=2),VarianceThreshold(),SelectKBest(k='all'),LDA(solver='svd',tol=0.00001))\n",
        "\n",
        "# Cross Validation\n",
        "#scores = cross_val_score(m,train,train_y.values.ravel(),n_jobs=-1)\n",
        "#print(\"%0.5f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
        "\n",
        "# Perform the fit, make the predictions and create the CSV\n",
        "print(m.fit(train, train_y.values.ravel()).score(test, test_y.values.ravel()))\n",
        "pY = pd.DataFrame(m.predict(df_test), index=range(1,len(df_test)+1), columns=['Class']) # ensure that labels and observations are in corresponding order\n",
        "df.Class.fillna('unknown').value_counts() # distribution of all train labels\n",
        "pY.value_counts() # distribution of predicted labels\n",
        "ToCSV(pY, 'MySubmission')"
      ],
      "metadata": {
        "id": "Nviw5N7GLTbn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb1f475a-fcf2-489f-e375-a17975e74d56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.97003125\n",
            "CPU times: user 51.3 s, sys: 4.35 s, total: 55.7 s\n",
            "Wall time: 52.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References:**"
      ],
      "metadata": {
        "id": "pzBsjCvS_kEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1]. Hands-On Machine Learning (HOML) with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems (2nd edition). Géron, A. (2019). O'Reilly Media ISBN-13: 978-1492032649\n",
        "\n",
        "[2]. Scikit-learn.org, 'API Reference — scikit-learn 1.2.2 documentation.' [Online]. Available: https://scikit-learn.org/stable/modules/classes.html"
      ],
      "metadata": {
        "id": "2kr8Q-9T_nAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=5>⌛</font> <strong><font color=orange size=5>Do not exceed competition's runtime limit!</font></strong>\n",
        "\n",
        "<hr color=red>\n"
      ],
      "metadata": {
        "id": "DoF2GoB_QGw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmr.ShowTime()    # measure Colab's runtime. Do not remove. Keep as the last cell in your notebook."
      ],
      "metadata": {
        "id": "bD1sdgYbNWQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e15fb6b9-37b5-4101-f8a7-4e6259fba155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime is 53 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUStTaN4uo_Z"
      },
      "source": [
        "## 💡**Starter Ideas**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Tune model hyperparameters\n",
        "1. Try to linear and non-linear feature normalization: shift/scale, log, divide features by features (investigate scatterplot matrix)\n",
        "1. Try higher order feature interactions and polynomial features on a small subsample. Then identify key features or select key principal components. The final model can be trained on a larger or even full training sample. You can use [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to reduce the feature set\n",
        "1. Do a thorough EDA: look for feature augmentations that result in linear decision boundaries between pairs of classes.\n",
        "1. Evaluate predictions and focus on poorly predicted \"groups\":\n",
        "  1. Strongest missclassifications. E.g. the model is very confident about the wrong label\n",
        "  1. Evaluate predictions near decision boundaries.\n",
        "1. Do scatter plots show piecewise linear shape? Can a separate linear model be used on each support, or can the pattern be linearized via transformations?\n",
        "1. How are date/categorical features treated by the model? Is there a [better way](https://www.google.com/search?q=ways+to+encode+categorical+data) to encode these (perhaps, ordinal) features?\n",
        "  1. E.g. you could replace codes (or groups of codes) with their frequencies, which may capture the implied \"distance\" or rarity between category levels.\n",
        "  1. If encoding ordinal features with integers, should non-equidistant values be considered?\n",
        "1. Learn astronomy domain and features: [🎦](https://www.youtube.com/results?search_query=Quasi-star), [quasi-star](https://en.wikipedia.org/wiki/Quasi-star), [star](https://en.wikipedia.org/wiki/Star), [galaxy](https://en.wikipedia.org/wiki/Galaxy), [📃](https://arxiv.org/abs/2112.02026)\n"
      ],
      "metadata": {
        "id": "q4QO-u3t8xAO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tLlIseR-EcrE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}