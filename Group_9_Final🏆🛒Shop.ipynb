{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsadaphule/jhu-dnn/blob/main/Group_9_Final%F0%9F%8F%86%F0%9F%9B%92Shop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3pqxgX4DxeH"
      },
      "source": [
        "<small><font color=gray>Notebook author: <a href=\"https://www.linkedin.com/in/olegmelnikov/\" target=\"_blank\">Oleg Melnikov</a> ©2021 onwards</font></small><hr style=\"margin:0;background-color:silver\">\n",
        "\n",
        "**[<font size=6>🛒Shop</font>](https://www.kaggle.com/competitions/26jun23jh-shop/rules)**. [**Instructions**](https://colab.research.google.com/drive/1riOGrE_Fv-yfIbM5V4pgJx4DWcd92cZr#scrollTo=ITaPDPIQEgXV) for running Colabs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QkDG8eYL75n"
      },
      "source": [
        "<small>**(Optional) CONSENT.** <mark>[ X ]</mark> We consent to sharing our Colab (after the assignment ends) with other students/instructors for educational purposes. We understand that sharing is optional and this decision will not affect our grade in any way. <font color=gray><i>(If ok with sharing your Colab for educational purposes, leave \"X\" in the check box.)</i></font></small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrogZ_8bD9tZ"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive; drive.mount('/content/drive')   # OK to enable, if your kaggle.json is stored in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8XoC8VqBXGs",
        "outputId": "f3ed026c-9917-42ba-9c8d-0f721dad1d61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "- competition is now set to: 26jun23jh-shop\n",
            "Using competition: 26jun23jh-shop\n",
            "  teamId  teamName               submissionDate       score    \n",
            "--------  ---------------------  -------------------  -------  \n",
            "10637586  Tony Sheller           2023-07-08 15:52:22  0.99324  \n",
            "10637379  Florian Muellerklein   2023-07-08 00:14:48  0.99300  \n",
            "10638614  Alayna Stepp           2023-07-06 01:34:40  0.99272  \n",
            "10631355  Harris Rose            2023-07-08 16:37:58  0.99264  \n",
            "10631144  Kewei Cai              2023-07-07 05:40:11  0.99228  \n",
            "10590634  kaggleintro            2023-07-05 01:42:02  0.99008  \n",
            "10638103  Group 15               2023-07-08 03:16:47  0.98588  \n",
            "10633911  Matthew Renze          2023-07-07 15:47:37  0.98500  \n",
            "10644064  Group 2                2023-07-08 08:53:50  0.98156  \n",
            "10637839  Larry Walker           2023-07-08 00:03:01  0.98124  \n",
            "10642100  Solomon Gruse          2023-07-07 03:06:27  0.98060  \n",
            "10651039  Philip-Spencer         2023-07-07 13:56:07  0.97556  \n",
            "10654893  NeilJoshi              2023-07-08 08:39:42  0.97332  \n",
            "10638955  Group 6                2023-07-08 00:07:42  0.96556  \n",
            "10638261  Alejandro Salamanca    2023-07-07 16:02:37  0.95196  \n",
            "10646999  Kain Place-Hildesheim  2023-07-08 02:04:34  0.94892  \n",
            "10635224  Margarita Prikhodko    2023-07-08 08:06:40  0.93256  \n",
            "10605620  Jon Pangia             2023-07-05 21:37:09  0.91440  \n",
            "10642310  Ravindra Sadaphule     2023-07-07 06:22:41  0.89184  \n",
            "10642846  binfant                2023-07-07 19:03:45  0.89172  \n",
            "10631498  Erik Wyatt-Nyquist     2023-07-08 00:08:28  0.87876  \n",
            "10638830  Cherie Magennis        2023-07-06 17:14:49  0.87204  \n",
            "10634431  JaneC                  2023-07-06 21:21:08  0.86968  \n",
            "10631514  Kevin Key              2023-07-07 03:59:33  0.86244  \n",
            "10633007  Nana Wang              2023-07-08 13:21:51  0.85496  \n",
            "10614691  Jeremy Shih            2023-07-07 20:40:21  0.84996  \n",
            "10639155  Jooyoung Park          2023-07-07 23:54:55  0.83416  \n",
            "10631821  Brian Yang             2023-07-08 05:57:51  0.83000  \n",
            "10633919  Mike Omelchenko        2023-07-08 05:02:50  0.82884  \n",
            "10647831  VLynch2                2023-07-06 06:07:57  0.82792  \n",
            "10631543  Lohith Muppala         2023-07-08 02:48:59  0.82428  \n",
            "10642713  Shawn Simon            2023-07-06 02:52:01  0.82144  \n",
            "10652071  Ryan Dombrowski        2023-07-08 06:42:30  0.82040  \n",
            "10647334  Chris Symons           2023-07-08 10:24:39  0.81952  \n",
            "10637941  Pratik Jasani          2023-07-07 21:55:58  0.81748  \n",
            "10634536  yjung2976              2023-07-05 20:39:55  0.81692  \n",
            "10637353  Vincent Kowalski       2023-07-08 07:24:09  0.81444  \n",
            "10634622  jake lock              2023-07-04 22:58:31  0.81416  \n",
            "10638175  Ben Dacek              2023-07-07 02:05:33  0.81384  \n",
            "10643380  Rebecca John           2023-07-06 15:11:30  0.81304  \n",
            "10650360  SCaitlynLee            2023-07-08 01:20:58  0.81080  \n",
            "10655073  Sanddhya Jayabalan     2023-07-08 15:09:55  0.80972  \n",
            "10642381  Kristina Moralic       2023-07-04 23:59:35  0.80460  \n",
            "10588281  🛒Baseline🐍             2023-06-21 13:13:26  0.80444  \n",
            "10632500  Matt Sunday            2023-07-02 08:46:27  0.80444  \n",
            "10635178  Lu Liu                 2023-07-07 00:27:31  0.80444  \n",
            "10654817  Noah Burkhardt         2023-07-07 23:31:40  0.80444  \n"
          ]
        }
      ],
      "source": [
        "!pip -q install --upgrade --force-reinstall --no-deps kaggle > log  # upgrade kaggle package (to avoid a warning)\n",
        "!mkdir -p ~/.kaggle                               # .kaggle folder must contain kaggle.json for kaggle executable to properly authenticate you to Kaggle.com\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json >log  # First, download kaggle.json from kaggle.com (in Account page) and place it in the root of mounted Google Drive\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json > log       # Alternative location of kaggle.json (without a connection to Google Drive)\n",
        "!chmod 600 ~/.kaggle/kaggle.json                  # give only the owner full read/write access to kaggle.json\n",
        "!kaggle config set -n competition -v 26jun23jh-shop    # set the competition context for the next few kaggle API calls. !kaggle config view - shows current settings\n",
        "!kaggle competitions download >> log              # download competition dataset as a zip file\n",
        "!unzip -o *.zip >> log                            # Kaggle dataset is copied as a single file and needs to be unzipped.\n",
        "!kaggle competitions leaderboard --show           # print public leaderboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Iie9fjWKwpV"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U tensorflow_addons > log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CyC-JlZFga1",
        "outputId": "8b3aa786-f1f3-4bf2-9473-0b3afabcc1bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 372 ms, sys: 2.77 ms, total: 375 ms\n",
            "Wall time: 378 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "%%capture\n",
        "%reset -f\n",
        "from IPython.core.interactiveshell import InteractiveShell as IS; IS.ast_node_interactivity = \"all\"\n",
        "import numpy as np, pandas as pd, time, matplotlib.pyplot as plt, seaborn as sns, tensorflow_addons as tfa\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import tensorflow as tf, tensorflow.keras as keras\n",
        "from keras.layers import Flatten, Dense\n",
        "ToCSV = lambda df, fname: df.round(2).to_csv(f'{fname}.csv', index_label='id') # rounds values to 2 decimals\n",
        "\n",
        "class Timer():\n",
        "  def __init__(self, lim:'RunTimeLimit'=60): self.t0, self.lim, _ = time.time(), lim, print(f'⏳ started. You have {lim} sec. Good luck!')\n",
        "  def ShowTime(self):\n",
        "    msg = f'Runtime is {time.time()-self.t0:.0f} sec'\n",
        "    print(f'\\033[91m\\033[1m' + msg + f' > {self.lim} sec limit!!!\\033[0m' if (time.time()-self.t0-1) > self.lim else msg)\n",
        "\n",
        "np.set_printoptions(linewidth=100, precision=2, edgeitems=2, suppress=True)\n",
        "pd.set_option('display.max_columns', 20, 'display.precision', 2, 'display.max_rows', 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "X00bQLb5FpxU",
        "outputId": "3adf3928-24b0-4210-c1f9-4055717ec735"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Adm  AdmDur  Inf  InfDur  Prd   PrdDur     BncRt     ExtRt  PgVal  \\\n",
              "0         0    0.00    0     0.0   18   132.99  3.82e-02  5.45e-02    0.0   \n",
              "1         1    0.00    0     0.0   37  1150.20  1.25e-03  3.03e-02    0.0   \n",
              "...     ...     ...  ...     ...  ...      ...       ...       ...    ...   \n",
              "499998    0    0.00    0     0.0   27  1185.14  0.00e+00  1.59e-03    0.0   \n",
              "499999    6   51.36    0     0.0   59  1898.21  0.00e+00  3.22e-03    0.0   \n",
              "\n",
              "        SpclDay  Mo  OS  Bsr  Rgn  TfcTp  VstTp  Wkd  Rev  \n",
              "0           0.0   4   3    1    1      2      0    1  NaN  \n",
              "1           0.0  11   2    2    4      2      0    1  NaN  \n",
              "...         ...  ..  ..  ...  ...    ...    ...  ...  ...  \n",
              "499998      0.0   5   2    2    2      3      0    1  0.0  \n",
              "499999      0.0  12   2    2    2      1      0    0  0.0  \n",
              "\n",
              "[500000 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88963010-b691-45f9-9cc1-a02289dcf12a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Adm</th>\n",
              "      <th>AdmDur</th>\n",
              "      <th>Inf</th>\n",
              "      <th>InfDur</th>\n",
              "      <th>Prd</th>\n",
              "      <th>PrdDur</th>\n",
              "      <th>BncRt</th>\n",
              "      <th>ExtRt</th>\n",
              "      <th>PgVal</th>\n",
              "      <th>SpclDay</th>\n",
              "      <th>Mo</th>\n",
              "      <th>OS</th>\n",
              "      <th>Bsr</th>\n",
              "      <th>Rgn</th>\n",
              "      <th>TfcTp</th>\n",
              "      <th>VstTp</th>\n",
              "      <th>Wkd</th>\n",
              "      <th>Rev</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18</td>\n",
              "      <td>132.99</td>\n",
              "      <td>3.82e-02</td>\n",
              "      <td>5.45e-02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37</td>\n",
              "      <td>1150.20</td>\n",
              "      <td>1.25e-03</td>\n",
              "      <td>3.03e-02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499998</th>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27</td>\n",
              "      <td>1185.14</td>\n",
              "      <td>0.00e+00</td>\n",
              "      <td>1.59e-03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499999</th>\n",
              "      <td>6</td>\n",
              "      <td>51.36</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59</td>\n",
              "      <td>1898.21</td>\n",
              "      <td>0.00e+00</td>\n",
              "      <td>3.22e-03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500000 rows × 18 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88963010-b691-45f9-9cc1-a02289dcf12a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-88963010-b691-45f9-9cc1-a02289dcf12a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-88963010-b691-45f9-9cc1-a02289dcf12a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "df = pd.read_csv('XY_Shop.csv'); df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFp0IV3BJR9_",
        "outputId": "2a2c2214-a243-4cb5-9fa7-c1944969bf4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500000 entries, 0 to 499999\n",
            "Data columns (total 18 columns):\n",
            " #   Column   Non-Null Count   Dtype  \n",
            "---  ------   --------------   -----  \n",
            " 0   Adm      500000 non-null  int64  \n",
            " 1   AdmDur   500000 non-null  float64\n",
            " 2   Inf      500000 non-null  int64  \n",
            " 3   InfDur   500000 non-null  float64\n",
            " 4   Prd      500000 non-null  int64  \n",
            " 5   PrdDur   500000 non-null  float64\n",
            " 6   BncRt    500000 non-null  float64\n",
            " 7   ExtRt    500000 non-null  float64\n",
            " 8   PgVal    500000 non-null  float64\n",
            " 9   SpclDay  500000 non-null  float64\n",
            " 10  Mo       500000 non-null  int64  \n",
            " 11  OS       500000 non-null  int64  \n",
            " 12  Bsr      500000 non-null  int64  \n",
            " 13  Rgn      500000 non-null  int64  \n",
            " 14  TfcTp    500000 non-null  int64  \n",
            " 15  VstTp    500000 non-null  int64  \n",
            " 16  Wkd      500000 non-null  int64  \n",
            " 17  Rev      450000 non-null  float64\n",
            "dtypes: float64(8), int64(10)\n",
            "memory usage: 68.7 MB\n"
          ]
        }
      ],
      "source": [
        "df.info()   # observe datatypes and any missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7OuVizOFsFF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53ed98f0-1d55-4903-d7df-e95a379a1db9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numeric features:  ['AdmDur', 'InfDur', 'PrdDur', 'BncRt', 'ExtRt', 'PgVal', 'SpclDay']\n"
          ]
        }
      ],
      "source": [
        "vX = df.query('Rev!=Rev').drop('Rev', axis=1)  # slice a test sample\n",
        "tXY = df.query('Rev==Rev')                     # slice training sample\n",
        "tX, tY = tXY.drop('Rev', axis=1), tXY.Rev      # split into training I/O\n",
        "NumFeatures = list(tX.select_dtypes(include='float').columns)\n",
        "print('Numeric features: ', NumFeatures)       # numeric/quantitative feature names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4gelET6Hb2A"
      },
      "outputs": [],
      "source": [
        "# def ScatterCorrHist(df):\n",
        "#   def corrdot(*args, **kwargs):\n",
        "#     # credit: https://stackoverflow.com/questions/48139899\n",
        "#     corr_r = args[0].corr(args[1], 'pearson')\n",
        "#     corr_text = f\"{corr_r:2.2f}\".replace(\"0.\", \".\")\n",
        "#     ax = plt.gca();\n",
        "#     ax.set_axis_off();\n",
        "#     msz = abs(corr_r) * 5000   # marker size\n",
        "#     fsz = abs(corr_r) * 40 + 5 # font size\n",
        "#     ax.scatter([.5], [.5], msz, [corr_r], alpha=0.5, cmap='coolwarm', vmin=-1, vmax=1, transform=ax.transAxes)\n",
        "#     ax.annotate(corr_text, [.5, .5,],  xycoords=\"axes fraction\", ha='center', va='center', fontsize=fsz)\n",
        "\n",
        "#   sns.set(style='white', font_scale=.8);\n",
        "#   g = sns.PairGrid(df, aspect=1, diag_sharey=False);\n",
        "#   g.fig.set_size_inches(20,10)\n",
        "#   g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color':'red'}, scatter_kws={'s':1});\n",
        "#   g.map_diag(sns.histplot, kde_kws={'color':'black'});\n",
        "#   g.map_upper(corrdot);\n",
        "#   g.fig.suptitle(\"Scatter plot, Correlations and histograms on diagonal\", y=1);\n",
        "#   _ = plt.subplots_adjust(hspace=0.02, wspace=0.02);\n",
        "#   _ = plt.show();\n",
        "\n",
        "# df_ = tXY.loc[(tXY[NumFeatures]>0).all(axis=1), NumFeatures+['Rev']].sample(n=100, random_state=0)\n",
        "# # df_ = df.select_dtypes(include='float').query('AdmDur>0 and InfDur>0 and PrdDur>0 and BncRt>0 and ExtRt>0 and PgVal>0 and SpclDay>0').sample(n=100, random_state=0)\n",
        "# ScatterCorrHist(df_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4_C58bbHuja",
        "outputId": "19c6c7f4-1706-4af6-b6af-8de928b1a1f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳ started. You have 60 sec. Good luck!\n"
          ]
        }
      ],
      "source": [
        "tmr = Timer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NcTKbw3KhAn"
      },
      "source": [
        "<hr color=red>\n",
        "\n",
        "<font size=5>⏳</font> <strong><font color=orange size=5>Your Code, Documentation, Ideas and Timer - All Start Here...</font></strong>\n",
        "\n",
        "**Student's Section** (between ⏳ symbols): add your code and documentation here."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FYI: This colab is being run on GPU.**"
      ],
      "metadata": {
        "id": "1HcG3tcyvlub"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4RGs3sNzCNi",
        "outputId": "88a2e864-51ff-4bec-83bf-3e83b510dcb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1 s, sys: 163 ms, total: 1.16 s\n",
            "Wall time: 1.23 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.utils import class_weight\n",
        "from copy import deepcopy\n",
        "\n",
        "#df = pd.read_csv('XY_Shop.csv')\n",
        "\n",
        "\n",
        "# Make New Columns Combining Information that is correlated\n",
        "def combineFeatures(df):\n",
        "    '''\n",
        "    Combine a few features\n",
        "    '''\n",
        "    pd.set_option('mode.chained_assignment', None)\n",
        "    df['AdmAdmDur'] = df.AdmDur/df.Adm\n",
        "    df['AdmAdmDur'] = df['AdmAdmDur'].fillna(0)\n",
        "\n",
        "    df['InfInfDur'] = df['InfDur']/df['Inf']\n",
        "    df['InfInfDur'] = df.InfInfDur.fillna(0)\n",
        "\n",
        "    df['PrdPrdDur'] = df['PrdDur']/df['Prd']\n",
        "    df['PrdPrdDur'] = df.PrdPrdDur.fillna(0)\n",
        "    #df.drop(['Adm','AdmDur','Inf','InfDur','Prd','PrdDur'],axis=1, inplace=True)\n",
        "    return df\n",
        "\n",
        "df = combineFeatures(deepcopy(df))\n",
        "\n",
        "\n",
        "\n",
        "# track which features are numeric and categorical\n",
        "numerical_feats = ['Adm', 'AdmDur','AdmAdmDur', 'Inf', 'InfDur','InfInfDur', 'PrdDur','PrdPrdDur', 'BncRt', 'ExtRt', 'PgVal', 'SpclDay', 'Prd']\n",
        "categorical_feats = ['OS', 'Bsr', 'Rgn', 'TfcTp', 'VstTp', 'Wkd', 'Mo']\n",
        "\n",
        "\n",
        "# get one hot encoded featuers without dropping originals\n",
        "df_dummies = pd.get_dummies(df[categorical_feats], columns=categorical_feats)\n",
        "df = pd.concat((df, df_dummies), axis=1)\n",
        "\n",
        "# make the categoricals all 0 indexed\n",
        "for cat_feat in categorical_feats:\n",
        "  df[cat_feat] = LabelEncoder().fit_transform(df[cat_feat])\n",
        "\n",
        "onehot_feats = [col_nm for col_nm in df.columns if '_' in col_nm]\n",
        "\n",
        "test_data = df.query('Rev!=Rev').drop('Rev', axis=1)                   # slice a test sample\n",
        "train_data = df.query('Rev==Rev')                                      # slice training sample\n",
        "train_x, train_y = train_data.drop('Rev', axis=1), train_data.Rev      # split into training I/O\n",
        "\n",
        "train_x[numerical_feats] = np.log(train_x[numerical_feats] + 1)\n",
        "test_data[numerical_feats] = np.log(test_data[numerical_feats] + 1)\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced', classes=train_y.unique(), y=train_y)\n",
        "\n",
        "#print('Numeric features: ', numerical_feats, '\\nCategorical features:', categorical_feats, '\\nOneHot features:', onehot_feats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpyLNt3god0c"
      },
      "source": [
        "## **Task 1. Preprocessing Pipeline**\n",
        "\n",
        "Explain elements of your preprocessing pipeline i.e. feature engineering, subsampling, clustering, dimensionality reduction, etc.\n",
        "1. Why did you choose these elements? (Something in EDA, prior experience,...? Btw, EDA is not required)\n",
        "1. How do you evaluate the effectiveness of these elements?\n",
        "1. What else have you tried that worked or didn't?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30xYIFXAnaPE"
      },
      "source": [
        "### Explain elements of your preprocessing pipeline i.e. feature engineering, subsampling, clustering, dimensionality reduction, etc.\n",
        "- The categorical features ['OS', 'Bsr', 'Rgn', 'TfcTp', 'VstTp', 'Wkd', 'Mo'] were both one hot encoded and category values re-encoded to be 0-indexed. This is due to the dual branch model architecture. The one hot encode categorical features are for the linear portion of our model to treat them all as independent discreet items. The re-encoding to be 0-indexed allows us to learn an embedding for each unique categorical value that attempts to encode the latent features of that category (similar to the goals of the SVD in the last competition). Those latent embedding features are then used as input to a DNN.\n",
        "- For the numerical features, the ones that are not categorical, we applied a log on the feature values + 1 since we noticed that they all have a positive skew.\n",
        "- Feature Engineering – Combined features and increased the score significantly. Specifically, we computed the average duration per page by calculating AdmDur/Adm, InfDur/Inf and PrdDur/Prd since we believed that the average time spent on a page per visit, rather than the total time across all visits might be a useful signal.\n",
        "\n",
        "### 1. Why did you choose these elements? (Perhaps something in EDA or prior experience lead you to these)\n",
        "- One hot encode categorical features: for the features ['OS', 'Bsr', 'Rgn', 'TfcTp', 'VstTp', 'Wkd', 'Mo'], we applied one hot encoding so that the model can interpret these as categories are not ordinal features. While the original features were numerical, we did not want the model to interpret these as ordered values since for many of these, they are unordered.\n",
        "- The 0-indexed versions of the categorical features are used to choose the corresponding latent embedding from the matrix of size `(n_categoricals, n_features)` where `n_categoricals` is the number of unique elements in a given category and `n_features` is a parameter we can tune. After transforming we just pull the embedding from the given index corresponding to the encoded feature number.\n",
        "- For the numerical features, the ones that are not categorical, we applied a log on the feature values + 1 since we noticed that they all have a positive skew. Applying the log helps to shift the values to have a more symmetrical distribution which can help the model learn patterns more easily.\n",
        "\n",
        "### 2. How do you evaluate the effectiveness of these elements?\n",
        "- The model was evaluated with stratified k fold cross validation using 5 folds. We decided on which feature engineering steps to include based on which ones helped improve the accuracy score on the validation sets.\n",
        "\n",
        "### 3 What else have you tried that worked or didn't?\n",
        "- We noticed that for Adm, Inf and Prd, for many observations, these values were non-zero i.e. there were visits on these pages but the corresponding durations AdmDur, InfDur and PrdDur were zero which seemed like a potential data logging issue. So for the observations where there were >0 visits but 0 duration, we made the visits zero as well. However in testing this transformation, it did not lead to any dramatic improvements in performance.\n",
        "\n",
        "- We noticed that for the numerical features ['Adm', 'AdmDur', 'Inf', 'InfDur', 'PrdDur', 'BncRt', 'ExtRt', 'PgVal', 'SpclDay', 'Prd'] many of these looked like they contained outliers in the long tail. Therefore we investigated approaches to remove outliers by evaluating the distributions and plots (histogram, boxplots). However, we noticed that the Kaggle test set exhibited very similar extreme values. This led us to conclude that these may not be outliers afterall and we should include these values so that the model could learn from these rare instances.\n",
        "- From looking at the pairplots between features, we noticed that the classes were clearly separable using the PgVal features. It was unclear from this plot how much the other features would be helpful in this prediction problem. Therefore we hypothesized that there might be some features that are redundant and we could potentially drop. To analyse this, we conducted a feature ablation study where we would drop a feature and then re-run the model to observe how doing so would impact the accuracy score. We would repeat this for every feature. The model we tested on had an accuracy of 0.8706. Based on the results (shown below), we can see that every ablation led to a drop in accuracy, some more than others: PgVal and Wkd led to the biggest drops to 0.8324 and 0.8524 respectively. While others led to only small drops: both Adm and InfDur.\n",
        "```\n",
        "dropped to 0.8702.\n",
        "# Drop Adm: 0.8702\n",
        "# Drop AdmDur: 0.8694\n",
        "# Drop Inf: 0.8659 *\n",
        "# Drop InfDur: 0.8702\n",
        "# Drop Prd: 0.8701\n",
        "# Drop PrdDur: 0.8697\n",
        "# Drop BncRt: 0.8678\n",
        "# Drop ExtRt: 0.8695\n",
        "# Drop PgVal: 0.8324 **\n",
        "# Drop SpclDay: 0.8665\n",
        "# Drop Mo: 0.8692\n",
        "# Drop OS: 0.8721\n",
        "# Drop Bsr: 0.8667\n",
        "# Drop Rgn: 0.8648 *\n",
        "# Drop TfcTp: 0.8686\n",
        "# Drop VstTp: 0.8676\n",
        "# Drop Wkd: 0.8524 **\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGJRwzqHob4o"
      },
      "source": [
        "## **Task 2. Modeling Approach**\n",
        "Explain your modeling approach, i.e. ideas you tried and why you thought they would be helpful.\n",
        "\n",
        "1. How did these decisions guide you in modeling?\n",
        "1. How do you evaluate the effectiveness of these elements?\n",
        "1. What else have you tried that worked or didn't?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi6ZjgtWnb58"
      },
      "source": [
        "### Explain your modeling approach, i.e. ideas you tried and why you thought they would be helpful.\n",
        "- The model is based on the architecture from “Wide and Deep Learning” [2]. We build a neural network with two branches. One branch is a shallow linear model and the other a deep neural network, each branch taking slightly different data. For the wide branch we use as inputs all of the numerical features as well as the one hot encoding for the categorical features. For the deep branch we take the numerical features and the `LabelEncoded` categorical features.\n",
        "\n",
        "  - **Deep**: The deep neural network is a three layer multi-layer perceptron with ReLU activation functions and BatchNormalization [3] between the layers. We take the same approach as [2] and decrease the number of units in each successive layer. The first layer has 1024 units, 512 in the second, and finally the last layer has 128.\n",
        "  - **Wide**: The wide portion of the network is just a simple logistic classifier using the input features as well as the 128 output features from the deep branch bringing the total inputs to __.\n",
        "- Additionally, the model was trained with a weighted loss function to combat the class imbalance. We weighted the majority class by 0.64 and the minority class by 2.23. This resulted in our predicted distributions matching the training distributions perfectly. 78% of our predictions are of class 0 and the remaining 22% of from class 1.\n",
        "- One challenge with this model was the long runtime using the default optimization parameters. In order to achieve the desired 60 seconds run time we had to increase the batch size significantly but as outlined in that paper that typically comes at the cost of generalization performance.  We followed the advice in “Train longer, generalize better: close the generalization gap in large batch training of neural networks” by Hoffer et al [3] and “Accurate, large minibatch sgd: Training imagenet in 1 hour.” Goyal et al. [4]. First we increase the learning rate by the proposed function in [3]. Next, we decay the learning rate twice towards the end of training (rather than doing the first near the middle as is typically done) also recommended in [3]. For example, if we train for 35 epochs, we decay the learning rate at 25 and 30 epochs. This allowed us to train a relatively complex model within the time limit without sacrificing too much accuracy.\n",
        "\n",
        "\n",
        "### 1. How did these decisions guide you in modeling?\n",
        "- These decisions were guided by increases in model performance based on score as well as being able to keep within the time constraints.\n",
        "- Wide and Deep networks was a topic of the modules reading and was very interesting to the group -- this interest guided us to evaluate its instead of a straightforward neural network.\n",
        "\n",
        "\n",
        "### 2. How do you evaluate the effectiveness of these elements?\n",
        "- The model was evaluated with stratified k fold cross validation using 5 folds. We decided that stratification was important due to the imbalance labels. We decided on which model to submit based on which model got the best average validation accuracy score while staying within the time limit.\n",
        "- The model was passsed to teammates over Microsoft teams with each gain in performance. This enabled teammates to conduct code reviews and validate the results. It also enableed them to pursue options for improevements that could be fed back in.\n",
        "\n",
        "### 3. What else have you tried that worked or didn't?\n",
        "- We tried various neural network architectures steadily increasing the complexity until we hit the architecture from [2]. We started with basic MLP using only numeric features, we then tested various sizes of that model with more units, layers, or both. Next we tested a variant of a deep and wide network similar to that in [1] (using one-hot encodings for both the wide and deep section, no embedding layers), again testing various sizes for the deep section there.\n",
        "- We also tried various learning rates, batch sizes, and number of epochs but we were ultimately restricted by runtime for some combinations that achieved better cross validation scores.\n",
        "Feature engineering was tried early and resulted in a score above the baseline.  As the DNN grew more sophisticated,  these additional features were abandoned as the focus was on refining and improving the model overall. After hyper-parameters were identified that optimized performance (and score), feature engineering was revisited and resulted in an increase in score.\n",
        "- Specific performance issues with the model hanging after the *fit* was researched [7]. The following code was added:\n",
        "\n",
        "```\n",
        "keras.layers.BatchNormalization._USE_V2_BEHAVIOR = False\n",
        "```\n",
        "\n",
        "\n",
        "- This value, by default is *True* and impacts the Batch Normalization Layer; the statistics resulting from training are using during testing or inference. When this value is *False*, a running average is used. This seems to fix the issue in most cases.\n",
        "\n",
        "- Additionally, `amsgrad=False` was added to the Adam optimizer. While this is the default, in testing, setting this proved to be beneficial in addressing issues with the model stalling after the fit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJs0jS4fIO1j"
      },
      "source": [
        "## Wide and Deep NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4LF9eVjyqYP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8b77bbf-947b-49e6-db82-3dd944aaadbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
            "Wall time: 10 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "def build_wnd_model(n_numeric, n_onehot, embed_dims=5):\n",
        "  '''\n",
        "  builds the model from Figure 4. in https://arxiv.org/abs/1606.07792\n",
        "  '''\n",
        "\n",
        "  # we'll need various inputs to track all the varous routing through the model\n",
        "  input_numericals = keras.layers.Input(shape=n_numeric)\n",
        "  input_onehots = keras.layers.Input(shape=n_onehot)\n",
        "\n",
        "  # input for each categorical variable for their respective embedding\n",
        "  input_os = keras.layers.Input(shape=(1,))\n",
        "  input_bsr = keras.layers.Input(shape=(1,))\n",
        "  input_rgn = keras.layers.Input(shape=(1,))\n",
        "  input_tfctp = keras.layers.Input(shape=(1,))\n",
        "  input_vsttp = keras.layers.Input(shape=(1,))\n",
        "  input_wkd = keras.layers.Input(shape=(1,))\n",
        "  input_mo = keras.layers.Input(shape=(1,))\n",
        "\n",
        "\n",
        "  # embeddings for each categorical variable\n",
        "  emb_os = keras.layers.Embedding(\n",
        "      input_dim=train_x['OS'].nunique() + 1, output_dim=embed_dims)(input_os)\n",
        "  emb_os = keras.layers.Flatten()(emb_os)\n",
        "\n",
        "  emb_bsr = keras.layers.Embedding(\n",
        "      input_dim=train_x['Bsr'].nunique() + 1, output_dim=embed_dims)(input_bsr)\n",
        "  emb_bsr = keras.layers.Flatten()(emb_bsr)\n",
        "\n",
        "  emb_rgn = keras.layers.Embedding(\n",
        "      input_dim=train_x['Rgn'].nunique() + 1, output_dim=embed_dims)(input_rgn)\n",
        "  emb_rgn = keras.layers.Flatten()(emb_rgn)\n",
        "\n",
        "  emb_tfctp = keras.layers.Embedding(\n",
        "      input_dim=train_x['TfcTp'].nunique() + 1, output_dim=embed_dims)(input_tfctp)\n",
        "  emb_tfctp = keras.layers.Flatten()(emb_tfctp)\n",
        "\n",
        "  emb_vsttp = keras.layers.Embedding(\n",
        "      input_dim=train_x['VstTp'].nunique() + 1, output_dim=embed_dims)(input_vsttp)\n",
        "  emb_vsttp = keras.layers.Flatten()(emb_vsttp)\n",
        "\n",
        "  emb_wkd = keras.layers.Embedding(\n",
        "      input_dim=train_x['Wkd'].nunique() + 1, output_dim=embed_dims)(input_wkd)\n",
        "  emb_wkd = keras.layers.Flatten()(emb_wkd)\n",
        "\n",
        "  emb_mo = keras.layers.Embedding(\n",
        "      input_dim=train_x['Mo'].nunique() + 1, output_dim=embed_dims)(input_mo)\n",
        "  emb_mo = keras.layers.Flatten()(emb_mo)\n",
        "\n",
        "  # the deep part uses the numerical data and the categorical embeddings\n",
        "  input_deep = keras.layers.Concatenate()([input_numericals, emb_os, emb_bsr, emb_rgn, emb_tfctp, emb_vsttp, emb_wkd, emb_mo])\n",
        "\n",
        "  # three layer deep network\n",
        "  bn_in = keras.layers.BatchNormalization()(input_deep)\n",
        "  hidden_a = keras.layers.Dense(\n",
        "      512,\n",
        "      activation=\"relu\",\n",
        "      kernel_initializer=\"he_normal\",\n",
        "      #kernel_regularizer=tf.keras.regularizers.L2(l2=1e-6),\n",
        "      name='hidden1_a'\n",
        "  )(bn_in)\n",
        "  bn_a = keras.layers.BatchNormalization()(hidden_a)\n",
        "  hidden_b = keras.layers.Dense(\n",
        "      256,\n",
        "      activation=\"relu\",\n",
        "      kernel_initializer=\"he_normal\",\n",
        "      #kernel_regularizer=tf.keras.regularizers.L2(l2=1e-6),\n",
        "      name='hidden_b'\n",
        "  )(bn_a)\n",
        "  bn_b = keras.layers.BatchNormalization()(hidden_b)\n",
        "  hidden_c = keras.layers.Dense(\n",
        "      128,\n",
        "      activation=\"relu\",\n",
        "      kernel_initializer=\"he_normal\",\n",
        "      #kernel_regularizer=tf.keras.regularizers.L2(l2=1e-6),\n",
        "      name='hidden_c'\n",
        "  )(hidden_b)\n",
        "  bn_c = keras.layers.BatchNormalization()(hidden_c)\n",
        "\n",
        "  # the wide part is the numericals and the onehots, combine with the deep output\n",
        "  concat = keras.layers.Concatenate()([input_numericals, input_onehots, bn_c])\n",
        "\n",
        "  # output pools all the above into a single prediction\n",
        "  output = keras.layers.Dense(1, activation=\"sigmoid\")(concat)\n",
        "\n",
        "  m = keras.Model(\n",
        "      inputs=[input_numericals, input_onehots, input_os, input_bsr, input_rgn, input_tfctp, input_vsttp, input_wkd, input_mo], outputs=[output])\n",
        "\n",
        "  return m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54INeiMDAGpW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76ff794b-313e-4742-8695-1ac1424ec762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 53 ms, sys: 0 ns, total: 53 ms\n",
            "Wall time: 71.7 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "tf.random.set_seed(0)   # always seed your experiments\n",
        "\n",
        "BS = 4096 * 3\n",
        "EPOCHS = 35\n",
        "base_lr = 0.002\n",
        "\n",
        "LR = np.sqrt(BS / 32) * base_lr # from https://arxiv.org/abs/1705.08741\n",
        "\n",
        "\n",
        "def lr_scheduler(epoch, lr):\n",
        "  # learning rate warmup: discussion paper uses gradient noise during this\n",
        "  # stage but I can't do that with keras so we borrow from https://arxiv.org/abs/1706.02677\n",
        "  # and do a learning rate warmup instead\n",
        "  if epoch == 0:\n",
        "    lr *= 0.1\n",
        "  # back to default\n",
        "  elif epoch == 1:\n",
        "    lr *= 10\n",
        "\n",
        "  if epoch in [25, 30]:\n",
        "    lr *= 0.1\n",
        "\n",
        "  return lr\n",
        "\n",
        "lr_sched = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "es = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "aayGjYjlzWYg",
        "outputId": "64dccdba-db2f-43da-f6b3-e505b956cd9d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n%%time\\nkf = StratifiedKFold(n_splits=5, random_state = 0,shuffle=True)\\nkf.get_n_splits(train_x, train_y)\\n\\nsplit_performance = []\\n\\nfor i, (train_index, test_index) in enumerate(kf.split(train_x, train_y)):\\n  # we don\\'t want the preprocessing to hold over between runs\\n  _train_x, _train_y = train_x.copy(), train_y.copy()\\n\\n  # start a fresh model for each fold\\n  m = build_wnd_model(len(numerical_feats), len(onehot_feats))\\n\\n  m.compile(\\n      loss=\"binary_crossentropy\",\\n      optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\\n      metrics=[\\'accuracy\\']\\n  )\\n\\n  #scaler = StandardScaler()\\n  #_train_x.iloc[train_index][numerical_feats] = scaler.fit_transform(_train_x.iloc[train_index][numerical_feats])\\n  #_train_x.iloc[test_index][numerical_feats] = scaler.transform(_train_x.iloc[test_index][numerical_feats])\\n\\n  hist = m.fit(\\n    [_train_x.iloc[train_index][numerical_feats],\\n     _train_x.iloc[train_index][onehot_feats]\\n    ] + [_train_x.iloc[train_index][cat_feat] for cat_feat in categorical_feats],\\n    _train_y.iloc[train_index],\\n    epochs=EPOCHS,\\n    validation_data=(([\\n        _train_x.iloc[test_index][numerical_feats],\\n        _train_x.iloc[test_index][onehot_feats]\\n      ] + [_train_x.iloc[test_index][cat_feat] for cat_feat in categorical_feats], _train_y.iloc[test_index])),\\n    batch_size=BS,\\n    class_weight={i: class_weights[i] for i in range(2)},\\n    callbacks=[lr_sched],\\n    verbose=False,\\n  )\\n  split_performance.append(hist.history[\\'val_accuracy\\'][-1])\\n\\nprint(\\'Mean Val Acc:\\', np.mean(split_performance))\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "'''\n",
        "%%time\n",
        "kf = StratifiedKFold(n_splits=5, random_state = 0,shuffle=True)\n",
        "kf.get_n_splits(train_x, train_y)\n",
        "\n",
        "split_performance = []\n",
        "\n",
        "for i, (train_index, test_index) in enumerate(kf.split(train_x, train_y)):\n",
        "  # we don't want the preprocessing to hold over between runs\n",
        "  _train_x, _train_y = train_x.copy(), train_y.copy()\n",
        "\n",
        "  # start a fresh model for each fold\n",
        "  m = build_wnd_model(len(numerical_feats), len(onehot_feats))\n",
        "\n",
        "  m.compile(\n",
        "      loss=\"binary_crossentropy\",\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  #scaler = StandardScaler()\n",
        "  #_train_x.iloc[train_index][numerical_feats] = scaler.fit_transform(_train_x.iloc[train_index][numerical_feats])\n",
        "  #_train_x.iloc[test_index][numerical_feats] = scaler.transform(_train_x.iloc[test_index][numerical_feats])\n",
        "\n",
        "  hist = m.fit(\n",
        "    [_train_x.iloc[train_index][numerical_feats],\n",
        "     _train_x.iloc[train_index][onehot_feats]\n",
        "    ] + [_train_x.iloc[train_index][cat_feat] for cat_feat in categorical_feats],\n",
        "    _train_y.iloc[train_index],\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(([\n",
        "        _train_x.iloc[test_index][numerical_feats],\n",
        "        _train_x.iloc[test_index][onehot_feats]\n",
        "      ] + [_train_x.iloc[test_index][cat_feat] for cat_feat in categorical_feats], _train_y.iloc[test_index])),\n",
        "    batch_size=BS,\n",
        "    class_weight={i: class_weights[i] for i in range(2)},\n",
        "    callbacks=[lr_sched],\n",
        "    verbose=False,\n",
        "  )\n",
        "  split_performance.append(hist.history['val_accuracy'][-1])\n",
        "\n",
        "print('Mean Val Acc:', np.mean(split_performance))\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xqaGYN20nM4"
      },
      "source": [
        "# Make Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgdsOTgA0nvM",
        "outputId": "bcdbc7da-7b5e-4f8e-fa20-dbdf4e5bf6ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/35\n",
            "37/37 [==============================] - 5s 23ms/step - loss: 0.5189 - accuracy: 0.7292 - lr: 0.0039\n",
            "Epoch 2/35\n",
            "37/37 [==============================] - 1s 20ms/step - loss: 0.4803 - accuracy: 0.7523 - lr: 0.0392\n",
            "Epoch 3/35\n",
            "37/37 [==============================] - 1s 20ms/step - loss: 0.4051 - accuracy: 0.8084 - lr: 0.0392\n",
            "Epoch 4/35\n",
            "37/37 [==============================] - 1s 25ms/step - loss: 0.3389 - accuracy: 0.8537 - lr: 0.0392\n",
            "Epoch 5/35\n",
            "37/37 [==============================] - 1s 27ms/step - loss: 0.2820 - accuracy: 0.8877 - lr: 0.0392\n",
            "Epoch 6/35\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.2280 - accuracy: 0.9147 - lr: 0.0392\n",
            "Epoch 7/35\n",
            "37/37 [==============================] - 1s 29ms/step - loss: 0.1884 - accuracy: 0.9318 - lr: 0.0392\n",
            "Epoch 8/35\n",
            "37/37 [==============================] - 1s 32ms/step - loss: 0.1572 - accuracy: 0.9453 - lr: 0.0392\n",
            "Epoch 9/35\n",
            "37/37 [==============================] - 1s 36ms/step - loss: 0.1382 - accuracy: 0.9528 - lr: 0.0392\n",
            "Epoch 10/35\n",
            "37/37 [==============================] - 1s 35ms/step - loss: 0.1148 - accuracy: 0.9607 - lr: 0.0392\n",
            "Epoch 11/35\n",
            "37/37 [==============================] - 1s 33ms/step - loss: 0.0999 - accuracy: 0.9665 - lr: 0.0392\n",
            "Epoch 12/35\n",
            "37/37 [==============================] - 1s 27ms/step - loss: 0.0854 - accuracy: 0.9713 - lr: 0.0392\n",
            "Epoch 13/35\n",
            "37/37 [==============================] - 1s 27ms/step - loss: 0.0804 - accuracy: 0.9730 - lr: 0.0392\n",
            "Epoch 14/35\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.0753 - accuracy: 0.9746 - lr: 0.0392\n",
            "Epoch 15/35\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.0714 - accuracy: 0.9756 - lr: 0.0392\n",
            "Epoch 16/35\n",
            "37/37 [==============================] - 1s 27ms/step - loss: 0.0558 - accuracy: 0.9817 - lr: 0.0392\n",
            "Epoch 17/35\n",
            "37/37 [==============================] - 1s 26ms/step - loss: 0.0532 - accuracy: 0.9821 - lr: 0.0392\n",
            "Epoch 18/35\n",
            "37/37 [==============================] - 1s 28ms/step - loss: 0.0530 - accuracy: 0.9820 - lr: 0.0392\n",
            "Epoch 19/35\n",
            "37/37 [==============================] - 1s 29ms/step - loss: 0.0519 - accuracy: 0.9825 - lr: 0.0392\n",
            "Epoch 20/35\n",
            "37/37 [==============================] - 1s 35ms/step - loss: 0.0471 - accuracy: 0.9842 - lr: 0.0392\n",
            "Epoch 21/35\n",
            "37/37 [==============================] - 1s 35ms/step - loss: 0.0412 - accuracy: 0.9861 - lr: 0.0392\n",
            "Epoch 22/35\n",
            "37/37 [==============================] - 1s 36ms/step - loss: 0.0384 - accuracy: 0.9871 - lr: 0.0392\n",
            "Epoch 23/35\n",
            "37/37 [==============================] - 1s 37ms/step - loss: 0.0361 - accuracy: 0.9877 - lr: 0.0392\n",
            "Epoch 24/35\n",
            "37/37 [==============================] - 1s 26ms/step - loss: 0.0348 - accuracy: 0.9883 - lr: 0.0392\n",
            "Epoch 25/35\n",
            "37/37 [==============================] - 1s 20ms/step - loss: 0.0400 - accuracy: 0.9863 - lr: 0.0392\n",
            "Epoch 26/35\n",
            "37/37 [==============================] - 1s 20ms/step - loss: 0.0258 - accuracy: 0.9915 - lr: 0.0039\n",
            "Epoch 27/35\n",
            "37/37 [==============================] - 1s 20ms/step - loss: 0.0152 - accuracy: 0.9953 - lr: 0.0039\n",
            "Epoch 28/35\n",
            "37/37 [==============================] - 1s 20ms/step - loss: 0.0137 - accuracy: 0.9956 - lr: 0.0039\n",
            "Epoch 29/35\n",
            "37/37 [==============================] - 1s 20ms/step - loss: 0.0129 - accuracy: 0.9958 - lr: 0.0039\n",
            "Epoch 30/35\n",
            "37/37 [==============================] - 1s 20ms/step - loss: 0.0122 - accuracy: 0.9960 - lr: 0.0039\n",
            "Epoch 31/35\n",
            "37/37 [==============================] - 1s 19ms/step - loss: 0.0116 - accuracy: 0.9962 - lr: 3.9192e-04\n",
            "Epoch 32/35\n",
            "37/37 [==============================] - 1s 20ms/step - loss: 0.0115 - accuracy: 0.9963 - lr: 3.9192e-04\n",
            "Epoch 33/35\n",
            "37/37 [==============================] - 1s 20ms/step - loss: 0.0114 - accuracy: 0.9963 - lr: 3.9192e-04\n",
            "Epoch 34/35\n",
            "37/37 [==============================] - 1s 20ms/step - loss: 0.0114 - accuracy: 0.9963 - lr: 3.9192e-04\n",
            "Epoch 35/35\n",
            "37/37 [==============================] - 1s 20ms/step - loss: 0.0113 - accuracy: 0.9964 - lr: 3.9192e-04\n",
            "CPU times: user 37.6 s, sys: 1.29 s, total: 38.9 s\n",
            "Wall time: 44.1 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8a9f36fd90>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "%%time\n",
        "keras.layers.BatchNormalization._USE_V2_BEHAVIOR = False\n",
        "#scaler = StandardScaler()\n",
        "#train_x[numerical_feats] = scaler.fit_transform(train_x[numerical_feats])\n",
        "#test_data[numerical_feats] = scaler.transform(test_data[numerical_feats])\n",
        "\n",
        "m = build_wnd_model(\n",
        "    len(numerical_feats),\n",
        "    len(onehot_feats)\n",
        ")\n",
        "\n",
        "m.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=LR,amsgrad=False),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "m.fit(\n",
        "  [train_x[numerical_feats],\n",
        "   train_x[onehot_feats]\n",
        "  ] + [train_x[cat_feat] for cat_feat in categorical_feats],\n",
        "  train_y,\n",
        "  epochs=EPOCHS,\n",
        "  batch_size=BS,\n",
        "  class_weight={i: class_weights[i] for i in range(2)},\n",
        "  callbacks=[lr_sched]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_In-QcBCCPC",
        "outputId": "862b7d2f-5538-48f2-926d-025052be6b4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 4s 2ms/step\n",
            "CPU times: user 5.37 s, sys: 252 ms, total: 5.62 s\n",
            "Wall time: 4.68 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Rev\n",
              "0      0.78\n",
              "1      0.22\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "%%time\n",
        "pY = pd.DataFrame(m.predict([test_data[numerical_feats], test_data[onehot_feats]] + [test_data[cat_feat] for cat_feat in categorical_feats]), index=test_data.index+1, columns=['Rev'])\n",
        "ToCSV(pY.round(0).astype(int), 'wide-n-deep')\n",
        "train_y.value_counts()/len(train_y)                # distribution of training target level\n",
        "pY.round(0).astype(int).value_counts()/len(pY)     # distribution of test target level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzBsjCvS_kEw"
      },
      "source": [
        "# **References:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kr8Q-9T_nAb"
      },
      "source": [
        "[1] Géron, A. (2019). Hands-on machine learning with Scikit-learn, Keras, and Tensorflow: Concepts, tools, and techniques to build intelligent systems (2nd edition). O'Reilly Media.\n",
        "\n",
        "[2] Cheng, H. T., Koc, L., Harmsen, J., Shaked, T., Chandra, T., Aradhye, H., ... & Shah, H. (2016, September). Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems (pp. 7-10).\n",
        "\n",
        "[3] Ioffe, S., & Szegedy, C. (2015, June). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning (pp. 448-456). pmlr.\n",
        "\n",
        "[4] Hoffer, E., Hubara, I., & Soudry, D. (2017). Train longer, generalize better: closing the generalization gap in large batch training of neural networks. Advances in neural information processing systems, 30.\n",
        "\n",
        "[5] Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., ... & He, K. (2017). Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677.\n",
        "\n",
        "[6] Scikit-learn.org, 'API Reference — scikit-learn 1.2.2 documentation.' [Online]. Available: https://scikit-learn.org/stable/modules/classes.html\n",
        "\n",
        "[7] Tensorflow. (n.d.). Hangs on model.fit · issue #36072 · Tensorflow/Tensorflow. GitHub. https://github.com/tensorflow/tensorflow/issues/36072\n",
        "\n",
        "[8] Tf.keras.optimizers.Adam  :  tensorflow V2.13.0. TensorFlow. (n.d.). https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoF2GoB_QGw9"
      },
      "source": [
        "<font size=5>⌛</font> <strong><font color=orange size=5>Do not exceed competition's runtime limit!</font></strong>\n",
        "\n",
        "<hr color=red>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD1sdgYbNWQA",
        "outputId": "7ba513b4-09c7-45a8-964e-38b77cec676e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime is 50 sec\n"
          ]
        }
      ],
      "source": [
        "tmr.ShowTime()    # measure Colab's runtime. Do not remove. Keep as the last cell in your notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUStTaN4uo_Z"
      },
      "source": [
        "## 💡**Starter Ideas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4QO-u3t8xAO"
      },
      "source": [
        "**Model**\n",
        "1. Tune model hyperparameters, batch size, optimizer, NN layers\n",
        "\n",
        "**Features**\n",
        "1. Try to linear and non-linear feature normalization: shift/scale, log, divide features by features (investigate scatterplot matrix)\n",
        "1. Try higher order feature interactions and polynomial features on a small subsample. Then identify key features or select key principal components. The final model can be trained on a larger or even full training sample. You can use [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to reduce the feature set\n",
        "1. Incorporate categorical features (appropriately encoded)\n",
        "  1. E.g. you could replace codes (or groups of codes) with their frequencies, which may capture the implied \"distance\" or rarity between category levels.\n",
        "  1. If encoding ordinal features with integers, should non-equidistant values be considered?\n",
        "\n",
        "**Training observations**\n",
        "1. Try clustering methods to remove similar observations. You may also try dimension reduction methods (eg. PCA) on the transposed data matrix (if it has scaled numeric features).\n",
        "1. Look for and deal with outliers or influential points in the training set\n",
        "1. Deal with **imbalanced sample**: oversample smaller class, or undersample larger class, or provide observation weights or provide class weights, or seek a suitable loss function\n",
        "1. Investigate distributions of features. Any missing values? Any zero values?\n",
        "\n",
        "**Predictions**\n",
        "1. Evaluate predictions and focus on poorly predicted \"groups\":\n",
        "  1. Strongest misclassifications. E.g. the model is very confident about the wrong label\n",
        "  1. Evaluate predictions near decision boundaries.\n",
        "\n",
        "**EDA and Domain Expertise**\n",
        "1. Do a thorough EDA: look for feature augmentations that result in linear decision boundaries between pairs of classes.\n",
        "1. Learn about the domain: how should output relate to features? How do month or weekend impact users' buying activity?\n",
        "  1. User Agent [&#127910;](https://www.youtube.com/results?search_query=user+agent+browser), Google Analytics [&#127910;](https://www.youtube.com/results?search_query=google+analytics), tracking online shopping intent [&#127910;](https://www.youtube.com/results?search_query=tacking+online+shopping+intent), [📄](https://scholar.google.com/scholar?q=tracking+online+shopping+intent)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}