{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "VveuuvNCQ7rX",
        "2BKdtJkcRIPm"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsadaphule/jhu-dnn/blob/main/%5BGroup_15_Final_Submission%5D_8%F0%9F%8F%86%E2%98%A2%EF%B8%8FRadon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RngKMAelr3-Q"
      },
      "source": [
        "<small><font color=gray>Notebook author: <a href=\"https://www.linkedin.com/in/olegmelnikov/\" target=\"_blank\">Oleg Melnikov</a> ©2021 onwards</font></small><hr style=\"margin:0;background-color:silver\">\n",
        "\n",
        "**[<font size=6>☢️Radon</font>](https://www.kaggle.com/c/17jul23jh-radon/rules)**. [**Instructions**](https://colab.research.google.com/drive/1riOGrE_Fv-yfIbM5V4pgJx4DWcd92cZr#scrollTo=ITaPDPIQEgXV) for running Colabs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<small>**(Optional) CONSENT.** <mark>[ X ]</mark> We consent to sharing our Colab (after the assignment ends) with other students/instructors for educational purposes. We understand that sharing is optional and this decision will not affect our grade in any way. <font color=gray><i>(If ok with sharing your Colab for educational purposes, leave \"X\" in the check box.)</i></font></small>"
      ],
      "metadata": {
        "id": "YMxLe6lpNHJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive; drive.mount('/content/drive')   # OK to enable, if your kaggle.json is stored in Google Drive"
      ],
      "metadata": {
        "id": "lUqrHtk9a031",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e0beb23-9a62-4b97-93ed-27f07da3a43e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle >> log  # upgrade kaggle package (to avoid a warning)\n",
        "!mkdir -p ~/.kaggle                               # .kaggle folder must contain kaggle.json for kaggle executable to properly authenticate you to Kaggle.com\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json >>log  # First, download kaggle.json from kaggle.com (in Account page) and place it in the root of mounted Google Drive\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json > log       # Alternative location of kaggle.json (without a connection to Google Drive)\n",
        "!chmod 600 ~/.kaggle/kaggle.json                  # give only the owner full read/write access to kaggle.json\n",
        "!kaggle config set -n competition -v 17jul23jh-radon # set the competition context for the next few kaggle API calls. !kaggle config view - shows current settings\n",
        "!kaggle competitions download >> log              # download competition dataset as a zip file\n",
        "!unzip -o *.zip >> log                            # Kaggle dataset is copied as a single file and needs to be unzipped.\n",
        "!kaggle competitions leaderboard --show           # print public leaderboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHAYCFFkbGCF",
        "outputId": "539fa15c-a633-47ae-db3c-4543a99f5127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "- competition is now set to: 17jul23jh-radon\n",
            "Using competition: 17jul23jh-radon\n",
            "  teamId  teamName              submissionDate       score      \n",
            "--------  --------------------  -------------------  ---------  \n",
            "10718785  Group 15              2023-07-30 00:34:06  23.05968   \n",
            "10720520  Group 8               2023-07-30 02:50:29  24.30620   \n",
            "10723884  Group 4               2023-07-30 12:46:17  27.27605   \n",
            "10728215  Jin C                 2023-07-30 05:31:11  27.32239   \n",
            "10717925  Sanddhya Jayabalan    2023-07-30 17:12:43  27.98797   \n",
            "10719289  JaneC                 2023-07-29 17:21:43  28.80044   \n",
            "10724015  erykbanatt            2023-07-28 22:47:08  28.83907   \n",
            "10718607  Matt Sunday           2023-07-30 02:50:40  30.39904   \n",
            "10727378  Group3                2023-07-30 16:46:55  33.75676   \n",
            "10717755  Group 6               2023-07-30 12:46:55  35.21138   \n",
            "10719741  Vincent Kowalski      2023-07-28 23:30:25  35.23181   \n",
            "10735853  Lu Liu                2023-07-28 23:56:41  35.23181   \n",
            "10719208  Margarita Prikhodko   2023-07-30 03:48:41  35.42214   \n",
            "10727995  VLynch2               2023-07-29 20:36:56  36.19026   \n",
            "10719308  Chuck de Sully        2023-07-28 23:01:41  36.24144   \n",
            "10718752  Kristina Moralic      2023-07-29 20:31:11  36.79611   \n",
            "10720133  Pratik Jasani         2023-07-30 15:17:58  38.27782   \n",
            "10717627  Erik Wyatt-Nyquist    2023-07-29 00:01:00  38.79232   \n",
            "10724365  kaggleintro           2023-07-30 06:04:29  40.12427   \n",
            "10720675  jake lock             2023-07-30 02:42:27  41.27311   \n",
            "10718839  yjung2976             2023-07-30 14:38:24  41.63874   \n",
            "10728726  Rylee Oquendo         2023-07-30 00:33:19  42.16671   \n",
            "10719900  Larry Walker          2023-07-30 17:37:56  44.61965   \n",
            "10737096  Jing Qian             2023-07-30 12:50:13  49.70520   \n",
            "10720027  Group 2               2023-07-27 00:06:41  49.85790   \n",
            "10724183  Lohith Muppala        2023-07-30 17:21:35  50.42500   \n",
            "10717664  Andrew Robbertz       2023-07-28 11:44:25  53.24574   \n",
            "10723209  Philip-Spencer        2023-07-29 19:25:42  54.24034   \n",
            "10719057  Group 7               2023-07-29 15:19:30  54.96009   \n",
            "10721344  Amin_Diab             2023-07-30 12:52:03  55.58924   \n",
            "10720285  Florian Muellerklein  2023-07-30 02:46:56  55.66102   \n",
            "10724099  Rebecca John          2023-07-29 19:04:10  55.78015   \n",
            "10722830  Chris Symons          2023-07-29 23:49:16  57.25395   \n",
            "10720082  Shawn Simon           2023-07-28 23:28:11  58.82048   \n",
            "10720229  Kevin Key             2023-07-30 02:23:25  63.90569   \n",
            "10720207  Matthew Renze         2023-07-29 17:02:27  64.53507   \n",
            "10735721  NeilJoshi             2023-07-29 07:51:33  69.09691   \n",
            "10722496  Alayna Stepp          2023-07-28 20:13:19  72.40846   \n",
            "10721988  Alejandro Salamanca   2023-07-29 15:25:07  77.93069   \n",
            "10651278  ☢️Baseline            2023-07-06 21:27:50  81.17314   \n",
            "10725997  Noah Burkhardt        2023-07-28 08:17:10  81.17314   \n",
            "10731255  Group 7a              2023-07-29 03:23:41  105.38275  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip list keras|grep keras"
      ],
      "metadata": {
        "id": "-R8wDP_9FF9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install keras==2.12.0 >> log\n",
        "# !pip list keras|grep keras"
      ],
      "metadata": {
        "id": "d_v6uTWAEhx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mfoxO3JKx3r"
      },
      "source": [
        "!pip install -U tfds-nightly tensorflow_addons tensorflow keras==2.12.0 >> log # downgrade keras to bypass tf_utils import error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OLEcBocdkiR",
        "outputId": "c17561e5-7351-4e0b-c1dc-705c636e62c7"
      },
      "source": [
        "%%time\n",
        "%%capture\n",
        "%reset -f\n",
        "from IPython.core.interactiveshell import InteractiveShell as IS; IS.ast_node_interactivity = \"all\"\n",
        "import numpy as np, pandas as pd, time, tensorflow_addons as tfa, tensorflow as tf, tensorflow.keras as keras, os\n",
        "from keras.layers import Flatten, Dense\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'; os.environ['TF_CUDNN_DETERMINISTIC'] = '1'; # allows seeding RNG on GPU\n",
        "ToCSV = lambda df, fname: df.round(2).to_csv(f'{fname}.csv', index_label='id') # rounds values to 2 decimals\n",
        "\n",
        "class Timer():\n",
        "  def __init__(self, lim:'RunTimeLimit'=60): self.t0, self.lim, _ = time.time(), lim, print(f'⏳ started. You have {lim} sec. Good luck!')\n",
        "  def ShowTime(self):\n",
        "    msg = f'Runtime is {time.time()-self.t0:.0f} sec'\n",
        "    print(f'\\033[91m\\033[1m' + msg + f' > {self.lim} sec limit!!!\\033[0m' if (time.time()-self.t0-1) > self.lim else msg)\n",
        "\n",
        "np.set_printoptions(linewidth=100, precision=2, edgeitems=2, suppress=True)\n",
        "pd.set_option('display.max_columns', 20, 'display.precision', 2, 'display.max_rows', 4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.1 s, sys: 449 ms, total: 3.55 s\n",
            "Wall time: 3.61 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw = pd.read_csv('XY_radon.csv'); df_raw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "MGUUojSZchvX",
        "outputId": "7984f02a-880e-425d-c26c-dac7c51e8119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Uppm   adjwt basement  cntyfips     county  dupflag  floor    lat  \\\n",
              "0      1.80   54.97        Y        59     MORTON        0      1  46.66   \n",
              "1      1.65  499.34        N        85  KOSCIUSKO        0      1  40.85   \n",
              "...     ...     ...      ...       ...        ...      ...    ...    ...   \n",
              "12571  0.44  394.07        Y         3      ANOKA        0      0  44.91   \n",
              "12572  2.71  157.82        Y        15     MOHAVE        0      0  36.01   \n",
              "\n",
              "          lon  pcterr  ...  stfips  stopdt  stoptm  stratum  typebldg wave  \\\n",
              "0     -101.39    7.76  ...      38   32206    1230        2         2    1   \n",
              "1      -86.22   55.02  ...      18   11497    1430        3         1   32   \n",
              "...       ...     ...  ...     ...     ...     ...      ...       ...  ...   \n",
              "12571  -92.86    9.40  ...      27   32110    1500        2         1    4   \n",
              "12572 -113.21   14.46  ...       4   11496    1330        1         1   38   \n",
              "\n",
              "      windoor    zip  zipflag    Y  \n",
              "0         NaN  58554        0  NaN  \n",
              "1         NaN  46580        0  NaN  \n",
              "...       ...    ...      ...  ...  \n",
              "12571     NaN  55303        0  8.6  \n",
              "12572     NaN  86403        0  1.9  \n",
              "\n",
              "[12573 rows x 27 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-6a206738-f604-455d-9e4d-99a25210093a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Uppm</th>\n",
              "      <th>adjwt</th>\n",
              "      <th>basement</th>\n",
              "      <th>cntyfips</th>\n",
              "      <th>county</th>\n",
              "      <th>dupflag</th>\n",
              "      <th>floor</th>\n",
              "      <th>lat</th>\n",
              "      <th>lon</th>\n",
              "      <th>pcterr</th>\n",
              "      <th>...</th>\n",
              "      <th>stfips</th>\n",
              "      <th>stopdt</th>\n",
              "      <th>stoptm</th>\n",
              "      <th>stratum</th>\n",
              "      <th>typebldg</th>\n",
              "      <th>wave</th>\n",
              "      <th>windoor</th>\n",
              "      <th>zip</th>\n",
              "      <th>zipflag</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.80</td>\n",
              "      <td>54.97</td>\n",
              "      <td>Y</td>\n",
              "      <td>59</td>\n",
              "      <td>MORTON</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>46.66</td>\n",
              "      <td>-101.39</td>\n",
              "      <td>7.76</td>\n",
              "      <td>...</td>\n",
              "      <td>38</td>\n",
              "      <td>32206</td>\n",
              "      <td>1230</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>58554</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.65</td>\n",
              "      <td>499.34</td>\n",
              "      <td>N</td>\n",
              "      <td>85</td>\n",
              "      <td>KOSCIUSKO</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>40.85</td>\n",
              "      <td>-86.22</td>\n",
              "      <td>55.02</td>\n",
              "      <td>...</td>\n",
              "      <td>18</td>\n",
              "      <td>11497</td>\n",
              "      <td>1430</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>NaN</td>\n",
              "      <td>46580</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12571</th>\n",
              "      <td>0.44</td>\n",
              "      <td>394.07</td>\n",
              "      <td>Y</td>\n",
              "      <td>3</td>\n",
              "      <td>ANOKA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>44.91</td>\n",
              "      <td>-92.86</td>\n",
              "      <td>9.40</td>\n",
              "      <td>...</td>\n",
              "      <td>27</td>\n",
              "      <td>32110</td>\n",
              "      <td>1500</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>55303</td>\n",
              "      <td>0</td>\n",
              "      <td>8.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12572</th>\n",
              "      <td>2.71</td>\n",
              "      <td>157.82</td>\n",
              "      <td>Y</td>\n",
              "      <td>15</td>\n",
              "      <td>MOHAVE</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>36.01</td>\n",
              "      <td>-113.21</td>\n",
              "      <td>14.46</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>11496</td>\n",
              "      <td>1330</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>NaN</td>\n",
              "      <td>86403</td>\n",
              "      <td>0</td>\n",
              "      <td>1.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12573 rows × 27 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a206738-f604-455d-9e4d-99a25210093a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-49585c92-d918-446f-b7d2-206793e6eb9a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-49585c92-d918-446f-b7d2-206793e6eb9a')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-49585c92-d918-446f-b7d2-206793e6eb9a button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6a206738-f604-455d-9e4d-99a25210093a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6a206738-f604-455d-9e4d-99a25210093a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah9dc6TRfSuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e2aa3f6-ed13-453a-b823-63a4358ac810"
      },
      "source": [
        "tmr = Timer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳ started. You have 60 sec. Good luck!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr color=red>\n",
        "\n",
        "<font size=5>⏳</font> <strong><font color=orange size=5>Your Code, Documentation, Ideas and Timer - All Start Here...</font></strong>\n",
        "\n",
        "**Student's Section** (between ⏳ symbols): add your code and documentation here."
      ],
      "metadata": {
        "id": "3NcTKbw3KhAn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpyLNt3god0c"
      },
      "source": [
        "## **Task 1. Preprocessing Approach**\n",
        "\n",
        "Explain elements of your preprocessing pipeline i.e. feature engineering, subsampling, clustering, dimensionality reduction, etc.\n",
        "1. Why did you choose these elements? (Something in EDA, prior experience,...? Btw, EDA is not required)\n",
        "1. How do you evaluate the effectiveness of these elements?\n",
        "1. What else have you tried that worked or didn't?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE: Notebook should be run using GPU**\n",
        "\n",
        "**1. Explain elements of your preprocessing pipeline i.e. feature engineering, subsampling, clustering, dimensionality reduction, etc.**\n",
        "\n",
        "Our preprocessing consists of outlier removal and feature engineering.\n",
        "\n",
        "**-Outlier removal:** First, we noticed that the pcterr value has larger values present in the training set but not the test set and decided to remove outliers by clipping the upper bound.\n",
        "\n",
        "**-Feature engineering:** We took the log of the pcterr feature since the original values exhibited a heavy positive skew. We also created new features such as lat_high, lon_high, basement_zero, stratum_one and region_one which create a binary variable based on the value of the respective feature.\n",
        "\n",
        "Given that the nature of the problem is highly dependent on the geographical location, we computed a global FIPS value based on the state and zip fips. In essence, we combined the two features together to contain both statewide and countywide information.\n",
        "\n",
        "Finally, we computed mean values (column-wise) for each county and cntyfips. For example, county_mean_uranium and cntyfips_mean_uranium are examples of how we used column wise mean values grouped on county and cntyfips.\n",
        "\n",
        "**2. Why did you choose these elements? (Perhaps something in EDA or prior experience lead you to these)**\n",
        "\n",
        "We noticed that the pcterr values have a large positive skew so taking the log of the pcterr makes it closer to a gaussian distribution, which is easier for the neural network to learn from.\n",
        "\n",
        "The feature that led to a sizable improvement in performance was a binary feature that represented log_pcterr = 0. We analyzed the mistakes being made by an earlier model, and in doing so, we noticed that log_pcterr and y were negatively correlated: lower log_pcterr had higher y values, with the exception of log_pcterr = 0, which had low y values. Once we added in this feature, we saw a noticeable jump in both CV and Kaggle LB scores.\n",
        "\n",
        "Also, we chose features based on the nature of the dataset. The dataset we are analyzing is heavily dependent on geographical location. Therefore, we calculated mean values based on the geographical locations. This brings in information about the surrounding area as well as the specific instance we are trying to analyze.\n",
        "\n",
        "**3. How do you evaluate the effectiveness of these elements?**\n",
        "\n",
        "We evaluated the effectiveness of these elements by trying various configurations and running 5-fold cross validation in order to compute the mean squared error, and logging the performance in our notebook. It was an iterative approach to find novel ways to engineer the new features. A lot of effort went into feature engineering because we noticed that the model performance did not vary drastically with different configurations of model depth or hyperparameters (see details in next section).\n",
        "\n",
        "**4 What else have you tried that worked or didn't?**\n",
        "\n",
        "We tried to keep the data as it is in the baseline model and fine tune the model configuration; however, we noticed that the original baseline model was already configured well. Therefore there was no real value gain from just updating our model. We had to take a look at the features and find novel ways to engineer our data.\n",
        "\n",
        "The Kaggle dataset offers some insights into the data and the problem at hand. The documentation mentioned that the way radon enters houses is through its contact at various points such as the basement and floor as two examples. Given this information we decided to one hot encode the floor - as opposed to the sequential ordering it originally.\n",
        "\n",
        "We also tried clustering the data based on geographical location such as the latitude, longitude, cntyfips and statefips. The idea was that we could cluster the database on its geographical location. Given the importance of geographical location for this dataset the goal was to capture more granular geographical location however it had negligible effect.\n",
        "\n",
        "We also followed the suggestion to include distance to Radon source as a feature [2]. We found this was only a useful feature after grouping by state; but ultimately, there was not a strong enough correlation or improvement in the accuracy score to include this feature. This weak correlation can be seen in the below heatmaps.\n"
      ],
      "metadata": {
        "id": "Ff7BG4Fwtnj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1W3E3dtKftxMni8rulVlrmf0HWeKw8vz-)"
      ],
      "metadata": {
        "id": "Cs5GEHk-1D0F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30xYIFXAnaPE"
      },
      "source": [
        "**Student's answer:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import BatchNormalization, Dropout, Activation\n",
        "import numpy as np, pandas as pd, tensorflow as tf, plotly.express as px, platform, plotly, os, time\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from joblib import parallel_backend\n",
        "\n",
        "# pd.set_option('display.max_rows', None)"
      ],
      "metadata": {
        "id": "sj-bHiFiYO1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import os\n",
        "tf.random.set_seed(42) # always seed your experiments\n",
        "tf.keras.utils.set_random_seed(42)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'; # allows seeding RNG on GPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgph3tMntbHr",
        "outputId": "e1ef8fcb-3921-4aaf-eb23-3410587cd841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 175 µs, sys: 27 µs, total: 202 µs\n",
            "Wall time: 210 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RUN: Create data"
      ],
      "metadata": {
        "id": "OJ2wg2OgXyUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_raw.drop(['windoor','zipflag'], axis=1)  # drop non-numeric columns\n",
        "vX = df.query('Y!=Y').drop('Y', axis=1)    # slice a test sample\n",
        "tXY = df.query('Y==Y')                     # slice training sample\n",
        "tX, tY = tXY.drop('Y', axis=1), tXY.Y      # split into training I/O\n",
        "\n",
        "# tXY.head()                  # train outputs"
      ],
      "metadata": {
        "id": "TziecM2HknAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RUN: Feature eng & Outlier removal"
      ],
      "metadata": {
        "id": "NNFbkB3wvr3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.mode.chained_assignment = None  # default='warn\n",
        "\n",
        "# Outlier removal\n",
        "tXY = tXY[tXY.pcterr<150]\n",
        "\n",
        "# Feature engineering: add new features\n",
        "tXY['log_pcterr'] = np.log(tXY['pcterr']+1)\n",
        "vX['log_pcterr'] = np.log(vX['pcterr']+1)\n",
        "\n",
        "tXY['basement_num'] = np.where(tXY.basement=='Y',1,0)\n",
        "vX['basement_num'] = np.where(vX.basement=='Y',1,0)\n",
        "\n",
        "tXY['fips'] = tXY['stfips'] * 1000 + tXY['cntyfips']\n",
        "vX['fips'] = vX['stfips'] * 1000 + vX['cntyfips']\n",
        "\n",
        "tXY['zero_log_pcterr'] = (tXY['log_pcterr']==0).astype('int')\n",
        "vX['zero_log_pcterr'] = (vX['log_pcterr']==0).astype('int')\n",
        "\n",
        "tXY['high_adjwt'] = (tXY['adjwt']>1700).astype('int')\n",
        "vX['high_adjwt'] = (vX['adjwt']>1700).astype('int')\n",
        "\n",
        "tXY['lon_low'] = (tXY['lon']<-105).astype('int')\n",
        "vX['lon_low'] = (vX['lon']<-105).astype('int')\n",
        "tXY['lon_high'] = (tXY['lon']>-82).astype('int')\n",
        "vX['lon_high'] = (vX['lon']>-82).astype('int')\n",
        "\n",
        "tXY['lat_high'] = (tXY['lat']>42.5).astype('int')\n",
        "vX['lat_high'] = (vX['lat']>42.5).astype('int')\n",
        "\n",
        "tXY['wave_low'] = (tXY['wave']==-1).astype('int')\n",
        "vX['wave_low'] = (vX['wave']==-1).astype('int')\n",
        "\n",
        "tXY['floor_zero'] = (tXY['floor']==0).astype('int')\n",
        "vX['floor_zero'] = (vX['floor']==0).astype('int')\n",
        "\n",
        "tXY['basement_zero'] = (tXY['basement_num']==0).astype('int')\n",
        "vX['basement_zero'] = (vX['basement_num']==0).astype('int')\n",
        "\n",
        "tXY['stratum_one'] = (tXY['stratum']==1).astype('int')\n",
        "vX['stratum_one'] = (vX['stratum']==1).astype('int')\n",
        "\n",
        "tXY['region_one'] = (tXY['region']==1).astype('int')\n",
        "vX['region_one'] = (vX['region']==1).astype('int')\n",
        "\n",
        "# tXY.shape"
      ],
      "metadata": {
        "id": "1q0t-J7o8enX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RUN: Derived features calculations"
      ],
      "metadata": {
        "id": "ypwem35L4yMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute new features by aggregating/ averaging at the county or cntyfips level\n",
        "def computations(df, col):\n",
        "  # Calculate % homes with basement at col level\n",
        "  has_basement = pd.DataFrame(df[col].value_counts()).reset_index()\n",
        "  has_basement.columns = [col, 'county_count']\n",
        "  temp_has_basement = pd.DataFrame(df[df.basement_num>0][col].value_counts()).reset_index()\n",
        "  temp_has_basement.columns = [col, 'has_base']\n",
        "\n",
        "  has_basement = has_basement.merge(temp_has_basement, on=col, how='left')\n",
        "  has_basement[col+'_pct_has_basement'] = has_basement['has_base'] / has_basement['county_count']\n",
        "  has_basement = has_basement.drop(['county_count', 'has_base'], axis=1).fillna(0)\n",
        "\n",
        "  # Calculate % homes with >0 floor at col level\n",
        "  has_floor = pd.DataFrame(df[col].value_counts()).reset_index()\n",
        "  has_floor.columns = [col, 'county_count']\n",
        "  temp_has_floor = pd.DataFrame(df[df.floor>0][col].value_counts()).reset_index()\n",
        "  temp_has_floor.columns = [col, 'has_floor']\n",
        "\n",
        "  has_floor = has_floor.merge(temp_has_floor, on=col, how='left')\n",
        "  has_floor[col+'_pct_has_floor'] = has_floor['has_floor'] / has_floor['county_count']\n",
        "  has_floor = has_floor.drop(['county_count', 'has_floor'], axis=1).fillna(0)\n",
        "\n",
        "  # Calculate avg uranum at col level\n",
        "  mean_uranium = df[[col,'Uppm']].groupby(col).mean().reset_index().fillna(0)\n",
        "  mean_uranium.columns = [col,col+'_mean_uranium']\n",
        "\n",
        "  # Calculate avg adjwt at col level\n",
        "  mean_adjwt = df[[col,'adjwt']].groupby(col).mean().reset_index().fillna(0)\n",
        "  mean_adjwt.columns = [col,col+'_mean_adjwt']\n",
        "\n",
        "  # NEW: Calculate avg pcterr at col level\n",
        "  mean_pcterr = df[[col,'pcterr']].groupby(col).mean().reset_index().fillna(0)\n",
        "  mean_pcterr.columns = [col,col+'_pcterr']\n",
        "  df = df.merge(mean_pcterr, on=col, how='left')\n",
        "\n",
        "  df = df.merge(has_basement, on=col, how='left')\n",
        "  df = df.merge(has_floor, on=col, how='left')\n",
        "  df = df.merge(mean_uranium, on=col, how='left')\n",
        "  df = df.merge(mean_adjwt, on=col, how='left')\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "D3BeXQZ043EM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tXY = computations(tXY, 'cntyfips')\n",
        "vX = computations(vX, 'cntyfips')\n",
        "tXY = computations(tXY, 'county')\n",
        "vX = computations(vX, 'county')\n",
        "# tXY.head()"
      ],
      "metadata": {
        "id": "w5cgRaXv_n2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGJRwzqHob4o"
      },
      "source": [
        "## **Task 2. Modeling Approach**\n",
        "Explain your modeling approach, i.e. ideas you tried and why you thought they would be helpful.\n",
        "\n",
        "1. How did these decisions guide you in modeling?\n",
        "1. How do you evaluate the effectiveness of these elements?\n",
        "1. What else have you tried that worked or didn't?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Explain your modeling approach, i.e. ideas you tried and why you thought they would be helpful.**\n",
        "\n",
        "We decided to use a neural network and after some experimentation (detailed below) observed that a small model would work best for this problem. This aligned with our expectations about how neural networks perform: given that there are only 6285 training observations and less than 50 features, a deep model would not work well for this problem. Therefore we started exploring networks with fewer parameters and settled on a model with only one hidden layer. This simple model combined with feature engineering performed significantly better than a more complex model without any feature engineering.\n",
        "\n",
        "We performed hyperparameter tuning on our model, testing different values for the number of neurons in the hidden layer, the dropout, batch size and learning rate. We found that the leaky relu activation function performed best and also utilized an exponential learning rate decay.\n",
        "We analyzed initializations to ensure they were configured correctly. For example, it was preferable to use He_Normal initialization when using a Relu or variation of Relu (leaky-relu). We were able to come to this conclusion based on our previous knowledge of neural networks and how they work.\n",
        "\n",
        "In the prediction outputs, we clip the lower bound of the predictions at 0; otherwise, the model outputs small negative values of y.\n",
        "\n",
        "**2. How did these decisions guide you in modeling?**\n",
        "\n",
        "The decisions mentioned above were largely based on our intuition on how neural networks performed with different datasets. As to which decisions we ended up going with, model performance and runtime were two main factors. For example, we knew based on the size of the data that a simpler neural network with fewer nodes than a deep neural network would perform just as well but also run much faster. This led us to experimenting with different hyperparameters of a smaller neural network as described above. In all, we were able to test a variety of hypotheses and see which performed best.\n",
        "\n",
        "**3. How do you evaluate the effectiveness of these elements?**\n",
        "\n",
        "We evaluated the effectiveness of these elements using 5 fold cross validation to assess how well the model performed on average as measured by the mean squared error. We plotted this in a learning plot over the epochs. We also plotted the mean absolute error as a guardrail metric. We recorded the performance of different techniques and approaches to see how various techniques ranked against one another.\n",
        "\n",
        "We also made submissions on Kaggle to see how our cross validation scores compared to the public leaderboard scores.\n",
        "\n",
        "**4. What else have you tried that worked or didn't?**\n",
        "\n",
        "The baseline model consisted of three hidden layers with 5 neurons per layer. At first we thought that a more complex model could perform better, so added more layers, more neurons, batch normalization layers and dropout layers. We tried hyperparameter tuning the model using random search over our parameter grid. Some of the parameters we tuned are batch size, epochs, learning rate, optimizers, dropout %, and number of neurons. However this did not provide significant lift in performance vs. the simple baseline model. From this, we realized that a more complex model does not necessarily perform better.  \n",
        "\n",
        "We observed that some of the predicted values were less than 0 so we had the idea of using a custom sigmoid activation function in the output layer to constrain the predictions, stretching the upper bound to 280 since this is the highest y value in the training data. However, we observed that while this did manage to cap the lowest values at zero, there were high errors for large y values. So we abandoned this approach in favor of our final approach."
      ],
      "metadata": {
        "id": "qvOrZG9QvuuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1T7fFcQjDHNkTLhNmwxJHvGegkXuX5Z_8)"
      ],
      "metadata": {
        "id": "HES9VUPw26EA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi6ZjgtWnb58"
      },
      "source": [
        "**Student's answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RUN: Define training data"
      ],
      "metadata": {
        "id": "ZqghHyFSVFTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## DEFINE TRAINING DATA\n",
        "include_cols = [\n",
        "    # Location\n",
        "    'fips',\n",
        "    # House\n",
        "    'floor',  'room', 'basement_num',\n",
        "    # Measures\n",
        "    'Uppm',  'log_pcterr',\n",
        "    # Other\n",
        "    'region', 'rep',\n",
        "    'stratum', 'typebldg', 'wave',\n",
        "    # Derived\n",
        "    'county_mean_uranium',\n",
        "    'cntyfips_mean_uranium',\n",
        "    'zero_log_pcterr', 'high_adjwt', 'lon_low', 'lon_high','lat_high', 'wave_low',\n",
        "    'floor_zero','basement_zero','stratum_one','region_one',\n",
        "    # NEW\n",
        "    'cntyfips_pcterr',\n",
        "    'county_pcterr',\n",
        "    ]\n",
        "input_X = tXY[include_cols]\n",
        "input_X.county_mean_uranium = input_X.county_mean_uranium.fillna(tXY.Uppm)\n",
        "input_X.cntyfips_mean_uranium = input_X.cntyfips_mean_uranium.fillna(tXY.Uppm)\n",
        "\n",
        "test_X = vX[include_cols]\n",
        "test_X.county_mean_uranium = test_X.county_mean_uranium.fillna(tXY.Uppm)\n",
        "test_X.cntyfips_mean_uranium = test_X.cntyfips_mean_uranium.fillna(tXY.Uppm)\n",
        "\n",
        "# NEW\n",
        "input_X.cntyfips_pcterr = input_X.cntyfips_pcterr.fillna(tXY.pcterr)\n",
        "input_X.county_pcterr = input_X.county_pcterr.fillna(tXY.pcterr)\n",
        "test_X.cntyfips_pcterr = test_X.cntyfips_pcterr.fillna(tXY.pcterr)\n",
        "test_X.county_pcterr = test_X.county_pcterr.fillna(tXY.pcterr)\n",
        "\n",
        "input_Y = tXY.Y\n",
        "\n",
        "# input_X.columns"
      ],
      "metadata": {
        "id": "WeCo83ZZuKWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RUN: Define NN model params"
      ],
      "metadata": {
        "id": "V0xeiq33SPFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 0.05\n",
        "NEURONS = 256\n",
        "BATCH_SIZE = 128\n",
        "DROPOUT = 0.4\n",
        "EPOCH = 30\n",
        "\n",
        "# Set a exponential decay for the learning rate\n",
        "def expDecayLR(eta0=LEARNING_RATE, s=10):\n",
        "  return lambda epoch: eta0 * 0.1**(epoch / s)  # returns a functional\n",
        "\n",
        "# # Define custom sigmoid activation function. 280 is max y value. 290\n",
        "# def sigmoid_stretched(x):\n",
        "#   return (1.0/(1 + tf.math.exp(-x))) * 290"
      ],
      "metadata": {
        "id": "nw3Iw0GwSHPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kfold CV"
      ],
      "metadata": {
        "id": "VveuuvNCQ7rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Kfold CV\n",
        "# %%time\n",
        "# num_folds = 5\n",
        "\n",
        "# X = input_X.reset_index(drop=True)\n",
        "# Y = input_Y.reset_index(drop=True)\n",
        "\n",
        "# # Stratified kfold\n",
        "# # kf = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
        "# # kf.get_n_splits(X, Y)\n",
        "\n",
        "# # Regular kfold\n",
        "# kf = KFold(n_splits=5, shuffle=True)\n",
        "# kf.get_n_splits(X)\n",
        "\n",
        "# # K-fold Cross Validation model evaluation\n",
        "# fold_no = 1\n",
        "# # Define per-fold score containers\n",
        "# split_performance = []\n",
        "# split_performance2 = []\n",
        "\n",
        "# # Loop over the dataset to create seprate folds\n",
        "# for train_index, test_index in kf.split(X): #, Y): # Need this for stratified kfold\n",
        "#   # print(test_index)\n",
        "#   X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "#   y_train, y_test = [Y[i] for i in train_index], [Y[i] for i in test_index]\n",
        "\n",
        "#   train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "#   validation_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "#   train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
        "#   validation_dataset = validation_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
        "\n",
        "#   tf.random.set_seed(0)   # always seed your experiments\n",
        "#   Init = keras.initializers.RandomNormal(seed=0)\n",
        "#   m = keras.models.Sequential([\n",
        "#       Flatten(input_shape=[input_X.shape[1]], name='input'),\n",
        "#       # input_X_norm,\n",
        "#       BatchNormalization(),\n",
        "#       Dense(NEURONS, activation=\"relu\", kernel_initializer=\"he_normal\", name='hidden1'),\n",
        "#       Dropout(DROPOUT),\n",
        "#       BatchNormalization(),\n",
        "#       Dense(1, activation=\"LeakyReLU\", kernel_initializer=Init, name='output')])\n",
        "\n",
        "#   compile_model = m.compile(\n",
        "#       loss=\"mse\",\n",
        "#       optimizer=tf.keras.optimizers.Adam(learning_rate=0.01, clipnorm=1.),\n",
        "#       metrics=['mean_absolute_error'])\n",
        "\n",
        "#   # Generate a print\n",
        "#   print('------------------------------------------------------------------------')\n",
        "#   print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "#   hist = m.fit(\n",
        "#       train_dataset,\n",
        "#       validation_data = validation_dataset,\n",
        "#       verbose = False,\n",
        "#       epochs = EPOCH,\n",
        "#       callbacks=[keras.callbacks.LearningRateScheduler(expDecayLR())])\n",
        "\n",
        "#   print(hist.history['val_mean_absolute_error'][-1])\n",
        "#   print(hist.history['val_loss'][-1])\n",
        "#   split_performance.append(hist.history['val_mean_absolute_error'][-1])\n",
        "#   split_performance2.append(hist.history['val_loss'][-1])\n",
        "\n",
        "#   # Increase fold number\n",
        "#   fold_no = fold_no + 1\n",
        "\n",
        "# print('------------------------------------------------------------------------')\n",
        "# print('Mean absolute error:', np.mean(split_performance), ', Var MAE =', np.var(split_performance))\n",
        "# print('Mean validation loss:', np.mean(split_performance2), ', Var loss =', np.var(split_performance2))"
      ],
      "metadata": {
        "id": "acu-wCBzRKm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run model & plot"
      ],
      "metadata": {
        "id": "2BKdtJkcRIPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(input_X, input_Y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "hDbghdsZ2qOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "\n",
        "# X_train\n",
        "# # Build NN model with hidden layers, using Relu activation, dropout and Batch norm\n",
        "# from joblib import parallel_backend\n",
        "\n",
        "# tf.random.set_seed(0)   # always seed your experiments\n",
        "# Init = keras.initializers.RandomNormal(seed=0)\n",
        "\n",
        "# with parallel_backend('threading'):\n",
        "#   m = keras.models.Sequential([\n",
        "#       Flatten(input_shape=[input_X.shape[1]], name='input'),\n",
        "#       BatchNormalization(),\n",
        "#       Dense(NEURONS, activation=\"relu\", kernel_initializer=\"he_normal\", name='hidden1'),\n",
        "#       Dropout(DROPOUT),\n",
        "#       BatchNormalization(),\n",
        "#       Dense(1, activation=\"LeakyReLU\", kernel_initializer=Init, name='output')])\n",
        "\n",
        "#   m.summary()\n",
        "#   compile_model = m.compile(\n",
        "#       loss=\"mse\",\n",
        "#       optimizer=tf.keras.optimizers.Adam(learning_rate=0.01, clipnorm=1.),\n",
        "#       metrics=['mean_absolute_error'])\n",
        "#   hist = m.fit(\n",
        "#       X_train,\n",
        "#       y_train,\n",
        "#       epochs = EPOCH,\n",
        "#       batch_size = BATCH_SIZE,\n",
        "#       validation_data = (X_test,y_test),\n",
        "#       # validation_split = 0.2,\n",
        "#       callbacks=[keras.callbacks.LearningRateScheduler(expDecayLR())])"
      ],
      "metadata": {
        "id": "mXM84E4BmTY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DConfig = {'modeBarButtonsToAdd':['drawline','drawopenpath','drawclosedpath','drawcircle','drawrect','eraseshape']} # more tool buttons for plotly\n",
        "\n",
        "# print(f'Hyperparameters: ', hist.params)\n",
        "# print(f'Epochs:          ', hist.epoch)\n",
        "# print(f'History:         ', hist.history)\n",
        "# dfHist = pd.DataFrame(hist.history, index=hist.epoch)  # create a datafrom from history dictionary\n",
        "# f = px.scatter(dfHist[['mean_absolute_error', 'val_mean_absolute_error']][2:])\n",
        "# f = f.update_traces(mode='lines+markers')\n",
        "# f = f.update_layout(width=800, height=400, margin=dict(l=0, r=0, t=30, b=0), title='Learning performance')\n",
        "# f.show(config=DConfig)\n",
        "\n",
        "# f2 = px.scatter(dfHist[['loss', 'val_loss']][2:])\n",
        "# f2 = f2.update_traces(mode='lines+markers')\n",
        "# f2 = f2.update_layout(width=800, height=400, margin=dict(l=0, r=0, t=30, b=0), title='Learning performance')\n",
        "# f2.show(config=DConfig)"
      ],
      "metadata": {
        "id": "_FRuID4cmTfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RUN: Train for submission"
      ],
      "metadata": {
        "id": "NiWTBzlg9Z-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Build NN model with hidden layers, using Relu activation, dropout and Batch norm\n",
        "from joblib import parallel_backend\n",
        "\n",
        "tf.random.set_seed(0)   # always seed your experiments\n",
        "Init = keras.initializers.RandomNormal(seed=0)\n",
        "\n",
        "with parallel_backend('threading'):\n",
        "  m = keras.models.Sequential([\n",
        "      Flatten(input_shape=[input_X.shape[1]], name='input'),\n",
        "      BatchNormalization(),\n",
        "      Dense(NEURONS, activation=\"relu\", kernel_initializer=\"he_normal\", name='hidden1'),\n",
        "      Dropout(DROPOUT),\n",
        "      BatchNormalization(),\n",
        "      Dense(1, activation=\"LeakyReLU\", kernel_initializer=Init, name='output')])\n",
        "\n",
        "  m.summary()\n",
        "  compile_model = m.compile(\n",
        "      loss=\"mse\",\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=0.01, clipnorm=1.),\n",
        "      metrics=['mean_absolute_error'])\n",
        "  hist = m.fit(\n",
        "      input_X,\n",
        "      input_Y,\n",
        "      epochs = EPOCH,\n",
        "      batch_size = BATCH_SIZE,\n",
        "      callbacks=[keras.callbacks.LearningRateScheduler(expDecayLR())])"
      ],
      "metadata": {
        "id": "-zsj2fqPmTlI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0ab4639-ea7f-4ceb-c61e-afc0d70d4274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input (Flatten)             (None, 25)                0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 25)               100       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " hidden1 (Dense)             (None, 256)               6656      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " output (Dense)              (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,037\n",
            "Trainable params: 7,475\n",
            "Non-trainable params: 562\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "50/50 [==============================] - 8s 13ms/step - loss: 59.6477 - mean_absolute_error: 2.9245 - lr: 0.0500\n",
            "Epoch 2/30\n",
            "50/50 [==============================] - 1s 13ms/step - loss: 46.6452 - mean_absolute_error: 2.2478 - lr: 0.0397\n",
            "Epoch 3/30\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 41.9185 - mean_absolute_error: 1.9047 - lr: 0.0315\n",
            "Epoch 4/30\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 46.5566 - mean_absolute_error: 1.9286 - lr: 0.0251\n",
            "Epoch 5/30\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 43.6896 - mean_absolute_error: 1.8871 - lr: 0.0199\n",
            "Epoch 6/30\n",
            "50/50 [==============================] - 1s 10ms/step - loss: 45.1188 - mean_absolute_error: 1.9964 - lr: 0.0158\n",
            "Epoch 7/30\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 39.9398 - mean_absolute_error: 1.8867 - lr: 0.0126\n",
            "Epoch 8/30\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 41.6786 - mean_absolute_error: 1.8577 - lr: 0.0100\n",
            "Epoch 9/30\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 38.8175 - mean_absolute_error: 1.7529 - lr: 0.0079\n",
            "Epoch 10/30\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 37.1643 - mean_absolute_error: 1.6946 - lr: 0.0063\n",
            "Epoch 11/30\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 35.3570 - mean_absolute_error: 1.6606 - lr: 0.0050\n",
            "Epoch 12/30\n",
            "50/50 [==============================] - 1s 10ms/step - loss: 40.5028 - mean_absolute_error: 1.7137 - lr: 0.0040\n",
            "Epoch 13/30\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 36.6848 - mean_absolute_error: 1.7156 - lr: 0.0032\n",
            "Epoch 14/30\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 36.1895 - mean_absolute_error: 1.6659 - lr: 0.0025\n",
            "Epoch 15/30\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 33.1251 - mean_absolute_error: 1.6623 - lr: 0.0020\n",
            "Epoch 16/30\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 35.0487 - mean_absolute_error: 1.6481 - lr: 0.0016\n",
            "Epoch 17/30\n",
            "50/50 [==============================] - 0s 10ms/step - loss: 35.7546 - mean_absolute_error: 1.6291 - lr: 0.0013\n",
            "Epoch 18/30\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 35.4467 - mean_absolute_error: 1.6721 - lr: 9.9763e-04\n",
            "Epoch 19/30\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 37.3022 - mean_absolute_error: 1.6268 - lr: 7.9245e-04\n",
            "Epoch 20/30\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 35.8499 - mean_absolute_error: 1.6188 - lr: 6.2946e-04\n",
            "Epoch 21/30\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 34.8999 - mean_absolute_error: 1.6717 - lr: 5.0000e-04\n",
            "Epoch 22/30\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 35.1673 - mean_absolute_error: 1.6230 - lr: 3.9716e-04\n",
            "Epoch 23/30\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 36.2106 - mean_absolute_error: 1.6134 - lr: 3.1548e-04\n",
            "Epoch 24/30\n",
            "50/50 [==============================] - 1s 13ms/step - loss: 36.0514 - mean_absolute_error: 1.6377 - lr: 2.5059e-04\n",
            "Epoch 25/30\n",
            "50/50 [==============================] - 1s 11ms/step - loss: 34.7171 - mean_absolute_error: 1.6346 - lr: 1.9905e-04\n",
            "Epoch 26/30\n",
            "50/50 [==============================] - 1s 11ms/step - loss: 38.2394 - mean_absolute_error: 1.6930 - lr: 1.5811e-04\n",
            "Epoch 27/30\n",
            "50/50 [==============================] - 1s 10ms/step - loss: 35.6201 - mean_absolute_error: 1.6419 - lr: 1.2559e-04\n",
            "Epoch 28/30\n",
            "50/50 [==============================] - 1s 12ms/step - loss: 33.5338 - mean_absolute_error: 1.6239 - lr: 9.9763e-05\n",
            "Epoch 29/30\n",
            "50/50 [==============================] - 1s 11ms/step - loss: 37.5332 - mean_absolute_error: 1.6343 - lr: 7.9245e-05\n",
            "Epoch 30/30\n",
            "50/50 [==============================] - 1s 12ms/step - loss: 34.0678 - mean_absolute_error: 1.5965 - lr: 6.2946e-05\n",
            "CPU times: user 15.8 s, sys: 1.53 s, total: 17.3 s\n",
            "Wall time: 23.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = m.predict(test_X)\n",
        "pred = pd.DataFrame(pred).clip(lower=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ef2stKw72rH",
        "outputId": "62fe02d3-1f35-437c-a7dc-70b07ef6260b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "197/197 [==============================] - 2s 6ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pY = pd.DataFrame(np.array(pred), index=np.arange(len(test_X))+1, columns=['y'])\n",
        "ToCSV(pY, 'kewei_radon_july_30_validation')"
      ],
      "metadata": {
        "id": "ZBZ7PwVY9sBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# files.download(\"kewei_radon_july_30_validation.csv\")"
      ],
      "metadata": {
        "id": "-5xyka7v9sEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pY.describe().T"
      ],
      "metadata": {
        "id": "dre7q7j99sID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pd.DataFrame(tXY.Y.describe()).T"
      ],
      "metadata": {
        "id": "4QKAiu4emToc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zFdToXh4RcBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kb71xzsdRcEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w36hA5sGRcHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H0wDauzARcK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QkKhcZrVRcN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = df_raw.select_dtypes(include=np.number).drop('windoor', axis=1)  # drop non-numeric columns\n",
        "# vX = df.query('Y!=Y').drop('Y', axis=1)    # slice a test sample\n",
        "# tXY = df.query('Y==Y')                     # slice training sample\n",
        "# tX, tY = tXY.drop('Y', axis=1), tXY.Y.astype(int)      # split into training I/O\n",
        "# print(tY.tolist()[:50])                    # train outputs"
      ],
      "metadata": {
        "id": "Uu_dha6Kc20u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjPZ1w_JdPea"
      },
      "source": [
        "# tf.random.set_seed(0)   # always seed your experiments\n",
        "# Init = keras.initializers.RandomNormal(seed=0)\n",
        "\n",
        "# m = keras.models.Sequential([\n",
        "#   Flatten(input_shape=[tX.shape[1]]),\n",
        "#   Dense(5, activation=\"relu\", kernel_initializer=Init),\n",
        "#   Dense(5, activation=\"relu\", kernel_initializer=Init),\n",
        "#   Dense(5, activation=\"relu\", kernel_initializer=Init),\n",
        "#   Dense(1, kernel_initializer=Init)])\n",
        "# m.summary()\n",
        "# m.compile(loss=\"mse\", optimizer=\"adam\", metrics=['mse'])\n",
        "# # m.compile(loss=tf.keras.losses.Huber(), optimizer=\"adam\", metrics=['mse'])\n",
        "\n",
        "# hist = m.fit(\n",
        "#     x=tX, y=tY,\n",
        "#     batch_size=32,\n",
        "#     epochs=5,\n",
        "#     validation_split=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fslr4WDXifWC"
      },
      "source": [
        "The model generates a baseline submission CSV file, see Colab folder (🗀 on the left), which you candownload and submit to Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lWV9-5HTnnm"
      },
      "source": [
        "# pY = pd.DataFrame(m.predict(vX), index=np.arange(len(vX))+1, columns=['y'])\n",
        "# ToCSV(pY.round(0).astype(int), 'Radon_Baseline')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References:**"
      ],
      "metadata": {
        "id": "pzBsjCvS_kEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Remember to cite your sources here as well! At the least, your textbook should be cited. Google Scholar allows you to effortlessly copy/paste an APA citation format for books and publications. Also cite StackOverflow, package documentation, and other meaningful internet resources to help your peers learn from these (and to avoid plagiarism claims)."
      ],
      "metadata": {
        "id": "2kr8Q-9T_nAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] Géron, A. (2019). Hands-on machine learning with Scikit-learn, Keras, and Tensorflow: Concepts, tools, and techniques to build intelligent systems (2nd edition). O'Reilly Media.\n",
        "\n",
        "[2] Stack Overflow. (2011). Haversine Formula in Python (Bearing and Distance between two GPS points). Retrieved July 21, 2023, from https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\n",
        "\n",
        "[3] Tensorflow. Multilevel Modeling Primer in TensorFlow Probability, https://www.tensorflow.org/probability/examples/Multilevel_Modeling_Primer\n",
        "\n",
        "[4] Stack Overflow. Training loss higher than validation loss. https://stackoverflow.com/questions/50387749/training-loss-higher-than-validation-loss\n",
        "\n",
        "[5] Stack Overflow. Restricting the output values of layers in Keras. https://stackoverflow.com/questions/44553722/restricting-the-output-values-of-layers-in-keras"
      ],
      "metadata": {
        "id": "av4wIZSV3Ilp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsfE60QEUbrr"
      },
      "source": [
        "<font size=5>⌛</font> <strong><font color=orange size=5>Do not exceed competition's runtime limit!</font></strong>\n",
        "\n",
        "<hr color=red>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs4wlpyUPxFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7d94b40-1922-4a03-bfa4-b3b9188268dc"
      },
      "source": [
        "tmr.ShowTime()    # measure Colab's runtime. Do not remove. Keep as the last cell in your notebook."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime is 28 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgH9_HiGk6uq"
      },
      "source": [
        "# **Starter Ideas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K732flZJlKNA"
      },
      "source": [
        "1. Try different DNN architectures and tuning of hyperparameters\n",
        "1. Try converting locations to distances to the key Radon sources (which you might need to discover).\n",
        "1. Try clustering categorical variables by their relation to Radon levels\n",
        "1. Try replacing categorical values with their level frequencies or other encodings\n",
        "1. Try scaling features linearly or nonlinearly\n",
        "1. Try embedding **textual** values (eg. US States names) with pre-trained SBERT-like models. This injects some additional information from Wikipedia (or whichever corpora were used for model training).\n",
        "1. Do EDA and understand the variables and their relation to the output. [Example 1](https://docs.pymc.io/en/v3.11.4/pymc-examples/examples/case_studies/multilevel_modeling.html), [Example 2](https://www.tensorflow.org/probability/examples/Multilevel_Modeling_Primer)\n",
        "\n",
        "<hr>\n",
        "<font color=black>\n",
        "    <details><summary><font color=carnelian>▶ </font>Clustering categorical variables <b></b>.</summary>\n",
        "\n",
        "  1. When we represent categorical variables as dummies, we may be losing important multivariate information. For example, say we use weekdays to predict the number of hours a person works. We could convert weekdays to 6 features (one is dropped due to collinearity). This requires 6 coefficients (degrees of freedom or sources of uncertainty). Essentially, we have an overparameterized model, whereas all we really need is two clusters of categorical values - weekends (Sat/Sun) and non-weekends (M/T/W/Th/F). In general, the model overparameterized model will do worse due to higher variance of the model output (resulting from the overfit and higher flexibility).\n",
        "\n",
        "  1. Here is another example from the NLP domain, where each word is a feature (or dimension). While morphological variants of a word (eg. run, running, runner, ran, runs, ...) have lower frequency, we cluster them into the same lemma \"run\", assuming only a small loss of semantic information. We hope that the gain in building a better distribution estimate for \"run\" is greater than the loss of semantic and lexical information.\n",
        "        </details>\n",
        "    <details><summary><font color=carnelian>▶ </font>Distance to Radon source<b></b>.</summary>\n",
        "\n",
        "If you can determine where Radon is most active (i.e. the source), then you might be able to compute the distance to the source. Ordinarily, we expect lower radiation for greater distance from the source (assuming uniform distribution of underground rivers, geology, rains/winds and other weather conditions affecting distribution of radon, etc.). You could also use categorical features in (e.g. US State, region, etc.), but these might perform better when clustered (again). Distance to the source is a real-valued feature, which does not require clustering.\n",
        "        </details>\n",
        "</font>"
      ]
    }
  ]
}