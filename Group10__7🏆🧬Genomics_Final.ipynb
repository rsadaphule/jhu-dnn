{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsadaphule/jhu-dnn/blob/main/Group10__7%F0%9F%8F%86%F0%9F%A7%ACGenomics_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbiR4YHvyX1f"
      },
      "source": [
        "<small><font color=gray>Notebook author: <a href=\"https://www.linkedin.com/in/olegmelnikov/\" target=\"_blank\">Oleg Melnikov</a> ©2021 onwards</font></small><hr style=\"margin:0;background-color:silver\">\n",
        "\n",
        "**[<font size=6>🧬Genomics</font>](https://www.kaggle.com/c/10jul23jh-genomics/rules)**. [**Instructions**](https://colab.research.google.com/drive/1riOGrE_Fv-yfIbM5V4pgJx4DWcd92cZr#scrollTo=ITaPDPIQEgXV) for running Colabs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<small>**(Optional) CONSENT.** <mark>[ X ]</mark> We consent to sharing our Colab (after the assignment ends) with other students/instructors for educational purposes. We understand that sharing is optional and this decision will not affect our grade in any way. <font color=gray><i>(If ok with sharing your Colab for educational purposes, leave \"X\" in the check box.)</i></font></small>"
      ],
      "metadata": {
        "id": "mwuPxmU5NFLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive; drive.mount('/content/drive')   # OK to enable, if your kaggle.json is stored in Google Drive"
      ],
      "metadata": {
        "id": "4ZK9-jiByA4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip -q install --upgrade --force-reinstall --no-deps kaggle > log  # upgrade kaggle package (to avoid a warning)\n",
        "# !mkdir -p ~/.kaggle                               # .kaggle folder must contain kaggle.json for kaggle executable to properly authenticate you to Kaggle.com\n",
        "# !cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json >log  # First, download kaggle.json from kaggle.com (in Account page) and place it in the root of mounted Google Drive\n",
        "# !cp kaggle.json ~/.kaggle/kaggle.json > log       # Alternative location of kaggle.json (without a connection to Google Drive)\n",
        "# !chmod 600 ~/.kaggle/kaggle.json                  # give only the owner full read/write access to kaggle.json\n",
        "# !kaggle config set -n competition -v 10jul23jh-genomics # set the competition context for the next few kaggle API calls. !kaggle config view - shows current settings\n",
        "# !kaggle competitions download >> log              # download competition dataset as a zip file\n",
        "# !unzip -o *.zip >> log                            # Kaggle dataset is copied as a single file and needs to be unzipped.\n",
        "# # !kaggle competitions leaderboard --show           # print public leaderboard"
      ],
      "metadata": {
        "id": "dt1AeSYEx5kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "%%capture\n",
        "%reset -f\n",
        "!pip -q install -U sentence-transformers > log\n",
        "!pip -q install -U tfds-nightly tensorflow > log\n",
        "!pip -q install -U sentence-transformers tensorflow_addons > log\n",
        "from IPython.core.interactiveshell import InteractiveShell as IS; IS.ast_node_interactivity = \"all\"\n",
        "import numpy as np, pandas as pd, time, matplotlib.pyplot as plt, os, plotly, tensorflow_addons as tfa, tensorflow as tf, tensorflow.keras as keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sentence_transformers import SentenceTransformer as SBERT\n",
        "from keras.layers import Flatten, Dense\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'; os.environ['TF_CUDNN_DETERMINISTIC'] = '1'; # allows seeding RNG on GPU\n",
        "ToCSV = lambda df, fname: df.round(2).to_csv(f'{fname}.csv', index_label='id') # rounds values to 2 decimals\n",
        "\n",
        "class Timer():\n",
        "  def __init__(self, lim:'RunTimeLimit'=60*5): self.t0, self.lim, _ = time.time(), lim, print(f'⏳ started. You have {lim} sec. Good luck!')\n",
        "  def ShowTime(self):\n",
        "    msg = f'Runtime is {time.time()-self.t0:.0f} sec'\n",
        "    print(f'\\033[91m\\033[1m' + msg + f' > {self.lim} sec limit!!!\\033[0m' if (time.time()-self.t0-1) > self.lim else msg)\n",
        "\n",
        "np.set_printoptions(linewidth=10000, precision=4, edgeitems=20, suppress=True)\n",
        "pd.set_option('display.max_rows', 100, 'display.max_columns', 100, 'display.max_colwidth', 100, 'display.precision', 2, 'display.max_rows', 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGUK_EcvtN8y",
        "outputId": "ea9ba1a6-9f83-4130-f446-997137642dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6.25 s, sys: 778 ms, total: 7.03 s\n",
            "Wall time: 40.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1TC9f43ItUY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "d49c3575-c722-4de8-ef18-52829e443386"
      },
      "source": [
        "vX = pd.read_csv('testX.csv').set_index('id')\n",
        "tYX = pd.read_csv('trainYX.csv').set_index('id')\n",
        "vX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                        DNA\n",
              "id                                                                                                         \n",
              "100000  TTGATTAATAAGATTCCTTGACACCCTTTGTAAAGTTTCTATTTCGTGTGAAATATCTATCTCTTCAAATCCTTTTAATTTATCTAGGTATTTGCT...\n",
              "100001  ATTAGTAACGGAGGATTTACTAGATGTTTGGATTTATATTCTAATTTTATTCAGGTGGAAGGGATTGTTTTATGATTCAATAGTATACAGAGAATA...\n",
              "...                                                                                                     ...\n",
              "119998  CGTCGGCATGCTCGGGCAGTGCGGCGGGCCAGCAGCGTGCCAGTTGTCGCGGGGCGGCCGGGCATCGCGGCGCCGGGCGGCAGCACTCCCGCGAAG...\n",
              "119999  GCGAGGGCACGAAGGCACGACGGCAACGGCGGCGAGGAGCGCTGTGGCAACCGTCTCCGCGTTTGCGTGCGTACAGCCGAGAGCTGGTTCGCGCAG...\n",
              "\n",
              "[20000 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-991b0029-926f-472c-b914-ced450912683\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DNA</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>100000</th>\n",
              "      <td>TTGATTAATAAGATTCCTTGACACCCTTTGTAAAGTTTCTATTTCGTGTGAAATATCTATCTCTTCAAATCCTTTTAATTTATCTAGGTATTTGCT...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100001</th>\n",
              "      <td>ATTAGTAACGGAGGATTTACTAGATGTTTGGATTTATATTCTAATTTTATTCAGGTGGAAGGGATTGTTTTATGATTCAATAGTATACAGAGAATA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119998</th>\n",
              "      <td>CGTCGGCATGCTCGGGCAGTGCGGCGGGCCAGCAGCGTGCCAGTTGTCGCGGGGCGGCCGGGCATCGCGGCGCCGGGCGGCAGCACTCCCGCGAAG...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119999</th>\n",
              "      <td>GCGAGGGCACGAAGGCACGACGGCAACGGCGGCGAGGAGCGCTGTGGCAACCGTCTCCGCGTTTGCGTGCGTACAGCCGAGAGCTGGTTCGCGCAG...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-991b0029-926f-472c-b914-ced450912683')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-ddc1a817-16cb-4af4-ba39-ecc85497236f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ddc1a817-16cb-4af4-ba39-ecc85497236f')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-ddc1a817-16cb-4af4-ba39-ecc85497236f button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-991b0029-926f-472c-b914-ced450912683 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-991b0029-926f-472c-b914-ced450912683');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JVbzlnuIud4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df03987-3b6d-4197-db3c-f7c16fe77998"
      },
      "source": [
        "tmr = Timer() # runtime limit (in seconds). Add all of your code after the timer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳ started. You have 300 sec. Good luck!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLxefmk0Iz0U"
      },
      "source": [
        "❗Do not modify the setup above."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr color=red>\n",
        "\n",
        "<font size=5>⏳</font> <strong><font color=orange size=5>Your Code, Documentation, Ideas and Timer - All Start Here...</font></strong>\n",
        "\n",
        "**Student's Section** (between ⏳ symbols): add your code and documentation here."
      ],
      "metadata": {
        "id": "3NcTKbw3KhAn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpyLNt3god0c"
      },
      "source": [
        "## **Task 1. Preprocessing Pipeline**\n",
        "\n",
        "Explain elements of your preprocessing pipeline i.e. feature engineering, subsampling, clustering, dimensionality reduction, etc.\n",
        "1. Why did you choose these elements? (Something in EDA, prior experience,...? Btw, EDA is not required)\n",
        "1. How do you evaluate the effectiveness of these elements?\n",
        "1. What else have you tried that worked or didn't?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30xYIFXAnaPE"
      },
      "source": [
        "  **Student's answer:**\n",
        "\n",
        "  1. When referencing the post by Ernest Bonat, Ph.D. and Bishes Rayamajhi, M.S. [here](https://medium.com/mlearning-ai/apply-machine-learning-algorithms-for-genomics-data-classification-132972933723), we noticed that k-mer counting was commonly used in bioinformatics and was used in one of their models. K-mers are subsequences of certain specified lengths (k) contained within a DNA sequence. For example, a subsequence of AGAT would have four monomers (A, G, A, and T) three 2-mers (AG, GA, AT), etc. We created frequency counts of how many times these k-mers appeared in the model, and then standardized the counts so that the feature counts would not dominate the model.\n",
        "\n",
        "  2. To evaluate the effectiveness of the preprocessing elements we were looking at the overall performance of the final model.  We chose to use the accuracy metric for this competition and using these added features we saw an improved performance in both validation accuracy and leaderboard accuracy.\n",
        "\n",
        "  3. We also tried OneHotEncoding the DNA sequence with limited success (~96% accuracy), so in the end we used the standardized k-mer counts and the encodings produced by SBERT model as our features."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# K-mer\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import warnings\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
        "\n",
        "bases = ['A', 'T', 'G', 'C']\n",
        "\n",
        "# Generate kmers for k=2, k=3, and k=4\n",
        "kmers = [''.join(p) for k in range(2, 5) for p in itertools.product(bases, repeat=k)]\n",
        "\n",
        "# Compute kmer counts for tYX and vX\n",
        "for kmer in kmers:\n",
        "    tYX[f'{kmer}_count'] = tYX['DNA'].str.count(kmer)\n",
        "    vX[f'{kmer}_count'] = vX['DNA'].str.count(kmer)"
      ],
      "metadata": {
        "id": "tXuxrDaDtDn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling\n",
        "# Use a more efficient method to scale the kmer counts\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "kmer_cols = [f'{kmer}_count' for kmer in kmers]\n",
        "scaler = StandardScaler()\n",
        "kmer_counts = scaler.fit_transform(tYX[kmer_cols])\n",
        "kmer_counts_val = scaler.transform(vX[kmer_cols])"
      ],
      "metadata": {
        "id": "iNTCO4ngxRj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGJRwzqHob4o"
      },
      "source": [
        "## **Task 2. Modeling Approach**\n",
        "Explain your modeling approach, i.e. ideas you tried and why you thought they would be helpful.\n",
        "\n",
        "1. How did these decisions guide you in modeling?\n",
        "1. How do you evaluate the effectiveness of these elements?\n",
        "1. What else have you tried that worked or didn't?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi6ZjgtWnb58"
      },
      "source": [
        "**Student's answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-w7CLJeJAZC"
      },
      "source": [
        "For our model approach, we fed the DNA sequence to [SBERT](https://www.sbert.net) to generate text embedding vectors for each sequence. We chose to 50 for the vector size of the sequences to have uniform vector length, though we also experimented with longer vector lengths. After converting the sequences using embeggings from SBERT and adding our k-mer coutns features, we fed the data to a neural network that had 3 dense layers with batch normalization and dropout after each layer.  The final layer was a dense layer with sigmoid activation.  Layers were initialized with \"He - Normal\" weights. The model itself was trained using a binary cross entropy loss function and utilized momentum, weight decay, and early stopping to avoid overfitting.\n",
        "1. From HOML and related course material, we know that initalization of layers can have a large effect on a neural network's ability to learn. Because of this, we experimented with different layer initialization techniques and found the He Normal initialization worked best.\n",
        "\n",
        "  In general, neural networks with more layers and more data tend to learn better. However, due to limited time constraints in these competitions it is often better to have smaller networks that can be trained quickly and that have quick inference times. We found that a 4 layer network (3 dense + 1 output) layer worked well. To help with regularization and avoid overfitting, we also added dropout and batchnormalization layers in-between the dense layers. The 3 hidden dense layers used the GELU activation.\n",
        "\n",
        "1. We evaluated the effectiveness of our model by using the validation accuracy with a validation split of 5% of the training data we had available. These validation scores were compared during different model runs to choose the best performing model.\n",
        "\n",
        "1. In addition to [SBERT](https://www.sbert.net) we also tried other embedding models such as USE as well as other training embeddings.  However, showed worse performance than SBERT using the [all-MiniLM-L12-v2](https://www.sbert.net/docs/pretrained_models.html) embedding. We also tried different vectorized embedding lengths, different number of layers, different layer activation functions, different layer initializations, learning rate schedules, and more. Overall, we found that the chosen model was a good balance between performance and training + inference time.\n",
        "\n",
        "  Another effort we took was to take a representative subsample of the raw input data before creating the embeddings and the k-mer processing to help keep the notebook runtime down, but it did not improve our overall performance in the end so this method was not employed for the final submission."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DNN Model Selection**\n",
        "\n",
        "**Neural Network Model**: Deeper and wider neural network can improve the perforamance overall, but if it is too deep or too wide, it starts to show overfitting. By checking training loss and validation loss graph, we controlled the number of nodes and the number of hidden layer. 3 hidden layers with 256, 512, 256 nodes show the efficient performance within 300 seconds. Also, adding batch normalization which can improve training stability and accelerate convergence is helpful to prevent overfitting. It normalizes the inputs of a layer by subtracting the batch mean and dividing by the batch standard deviation to increase stability.\n",
        "\n",
        "**Activation Function** is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron. That is exactly what an activation function does in an ANN as well. It takes in the output signal from the previous cell and converts it into some form that can be taken as input to the next cell. So, it is important to choose a proper activation function. We tested multiple different functions such as ReLU, Swish, Sigmoid,Tahn, LeakyReLU, but Gelu shows the best performance.\n",
        "\n",
        "**Optimizer** is an algorithm that adapts the neural network's attributes, like learning rate and weights. So, it helps in improving the accuracy and reduces the total loss. We tried multiple optimizer such as SGD, Adagrad, Adadelta, and Rmsprop, but Adam shows the best performance. In Adam, there are multiple parameters to control. Based on experiments, we found the best parameters for better performance.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Training parameters such as epoch, learning rate, batch size are important to control model's performance.\n",
        "\n",
        "**Epoch**: An epoch is a complete pass through the entire training dataset during the training process. In other words, it represents the number of times the model has seen and learned from the entire dataset. Training a model for multiple epochs allows it to refine its performance, but if epoch is too high, it starts to show overfitting. To avoid overfitting, we utilzed earlystopping method which stops epoch before overfitting based on validation loss. It works good to avoid oeverfitting.\n",
        "\n",
        "**Learning Rate**: The learning rate is a hyperparameter deciding the step size at which the model's parameters are updated during the optimization process. It controls how much the parameters should be adjusted in response to the estimated error. A large learning rate causes the parameters to update too quickly and a small learning rate causes the convergence process to be very slow. So, it is important to use an appropriate learning rate for training within limited time. Based on experiments, we decide to use 0.0076.\n",
        "\n",
        "**Batch Size**: During model training, the dataset is divided into small batches. The batch size refers to the number of samples that are processed together before the model's parameters are updated. The choice of batch size can have an impact on the model's computational efficiency. Generally, large batch size sometimes decreases model performance, but this binary classification is not difficult analysis. So, we decided to use large batch size to reduce time consuming."
      ],
      "metadata": {
        "id": "7-2jfyYPqad5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_586zfPPJD4C"
      },
      "source": [
        "# SBERT\n",
        "%%capture\n",
        "sbert = SBERT('all-MiniLM-L12-v2')\n",
        "%time tEmb, vEmb = sbert.encode([s[:50] for s in tYX.DNA]), sbert.encode([s[:50] for s in vX.DNA])  # embed all sequences to same-size vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tEmb = pd.DataFrame(tEmb)\n",
        "df_tEmb.reset_index(drop=True, inplace=True)\n",
        "df_kmer = pd.DataFrame(kmer_counts)\n",
        "df_kmer.reset_index(drop=True, inplace=True)\n",
        "tEmb_transformed = pd.concat([df_tEmb, df_kmer], axis=1)\n",
        "tEmb_transformed.columns = tEmb_transformed.columns.astype(str)\n",
        "\n",
        "df_vEmb = pd.DataFrame(vEmb)\n",
        "df_vEmb.reset_index(drop=True, inplace=True)\n",
        "df_kmer_v = pd.DataFrame(kmer_counts_val)\n",
        "df_kmer_v.reset_index(drop=True, inplace=True)\n",
        "vEmb_transformed = pd.concat([df_vEmb, df_kmer_v], axis=1)\n",
        "vEmb_transformed.columns = vEmb_transformed.columns.astype(str)"
      ],
      "metadata": {
        "id": "waEXTHxvxqCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normal DNN\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Input, Add, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "tf.random.set_seed(0)\n",
        "#Init = keras.initializers.RandomNormal(seed=0)\n",
        "\n",
        "kerinit = 'he_normal' #'he_normal'\n",
        "bs = 256\n",
        "ep = 25\n",
        "vs = 0.05\n",
        "LR = 0.0076\n",
        "\n",
        "# 256 512 256\n",
        "m = keras.models.Sequential([\n",
        "    Flatten(input_shape=[tEmb_transformed.shape[1]], name='input'),\n",
        "    Dense(256, activation='gelu', kernel_initializer=kerinit, name='hidden1'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(512, activation='gelu', kernel_initializer=kerinit, name='hidden2'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(256, activation='gelu', kernel_initializer=kerinit, name='hidden3'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid', kernel_initializer=kerinit, name='output')])\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
        "m.summary()\n",
        "m.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=LR,\n",
        "    beta_1=0.7, #0.7\n",
        "    beta_2=0.95,\n",
        "    epsilon=1e-07,\n",
        "    amsgrad=True,\n",
        "    weight_decay=0.02, #0.02\n",
        "    clipnorm=0.5, #0.5\n",
        "    use_ema=True,\n",
        "    ema_momentum=0.98), metrics=['accuracy'])\n",
        "\n",
        "hist = m.fit(tEmb_transformed, np.array(tYX.Y), batch_size=bs, epochs=ep, validation_split=vs, callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "W9X0uJ5bo499",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbceb36d-50f3-42ea-b7b0-73c0b7748718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input (Flatten)             (None, 720)               0         \n",
            "                                                                 \n",
            " hidden1 (Dense)             (None, 256)               184576    \n",
            "                                                                 \n",
            " batch_normalization_15 (Ba  (None, 256)               1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " hidden2 (Dense)             (None, 512)               131584    \n",
            "                                                                 \n",
            " batch_normalization_16 (Ba  (None, 512)               2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " hidden3 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_17 (Ba  (None, 256)               1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " output (Dense)              (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 451841 (1.72 MB)\n",
            "Trainable params: 449793 (1.72 MB)\n",
            "Non-trainable params: 2048 (8.00 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "372/372 [==============================] - 11s 14ms/step - loss: 0.0645 - accuracy: 0.9768 - val_loss: 0.0413 - val_accuracy: 0.9852\n",
            "Epoch 2/25\n",
            "372/372 [==============================] - 4s 10ms/step - loss: 0.0502 - accuracy: 0.9825 - val_loss: 0.0413 - val_accuracy: 0.9852\n",
            "Epoch 3/25\n",
            "372/372 [==============================] - 4s 10ms/step - loss: 0.0476 - accuracy: 0.9827 - val_loss: 0.0395 - val_accuracy: 0.9856\n",
            "Epoch 4/25\n",
            "372/372 [==============================] - 4s 12ms/step - loss: 0.0442 - accuracy: 0.9838 - val_loss: 0.0398 - val_accuracy: 0.9854\n",
            "Epoch 5/25\n",
            "372/372 [==============================] - 5s 13ms/step - loss: 0.0430 - accuracy: 0.9846 - val_loss: 0.0415 - val_accuracy: 0.9860\n",
            "Epoch 6/25\n",
            "372/372 [==============================] - 4s 10ms/step - loss: 0.0421 - accuracy: 0.9851 - val_loss: 0.0393 - val_accuracy: 0.9876\n",
            "Epoch 7/25\n",
            "372/372 [==============================] - 4s 10ms/step - loss: 0.0395 - accuracy: 0.9860 - val_loss: 0.0389 - val_accuracy: 0.9866\n",
            "Epoch 8/25\n",
            "372/372 [==============================] - 4s 10ms/step - loss: 0.0388 - accuracy: 0.9860 - val_loss: 0.0377 - val_accuracy: 0.9866\n",
            "Epoch 9/25\n",
            "372/372 [==============================] - 5s 13ms/step - loss: 0.0378 - accuracy: 0.9864 - val_loss: 0.0392 - val_accuracy: 0.9872\n",
            "Epoch 10/25\n",
            "372/372 [==============================] - 4s 12ms/step - loss: 0.0372 - accuracy: 0.9864 - val_loss: 0.0428 - val_accuracy: 0.9850\n",
            "Epoch 11/25\n",
            "372/372 [==============================] - 4s 10ms/step - loss: 0.0353 - accuracy: 0.9873 - val_loss: 0.0386 - val_accuracy: 0.9872\n",
            "Epoch 12/25\n",
            "372/372 [==============================] - 4s 10ms/step - loss: 0.0353 - accuracy: 0.9875 - val_loss: 0.0389 - val_accuracy: 0.9876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1PsK7LZJIgQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "172a5309-6657-41cb-ec00-fb81dfd3d076"
      },
      "source": [
        "pY = pd.DataFrame(m.predict(vEmb_transformed), index=vX.index, columns=['y'])   # predicted targets\n",
        "ToCSV((pY>0.5)*1, 'MySubmission_Final_v1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "625/625 [==============================] - 2s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References:**"
      ],
      "metadata": {
        "id": "pzBsjCvS_kEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Géron, Aurélien. Hands-on Machine Learning with Scikit-Learn, Keras, and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems, O’Reilly Media, Inc., Sebastopol, CA, 2019.\n",
        "- Bonat, Ernest and Rayamajhi, Bishes Rayamajhi. Apply Machine Learning Algorithms for Genomics Data Classification, Medium, 2021. https://medium.com/mlearning-ai/apply-machine-learning-algorithms-for-genomics-data-classification-132972933723\n",
        "- inspectorG4dget. Generating all DNA kmers with Python. StackOverflow, 2014. https://stackoverflow.com/questions/25942528/generating-all-dna-kmers-with-python\n",
        "- sklearn.preprocessing.StandardScaler. (n.d.). Scikit-learn. https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        "- Reimers, Nils. SentenceTransformers Documentation, SBERT.net, 2022. https://www.sbert.net/\n",
        "- Reimers, Nils. Pretrained Models, SBERT.net, 2022. https://www.sbert.net/docs/pretrained_models.html\n",
        "- He, Kaiming, et al. Delving Deep into Rectifiers:\n",
        "Surpassing Human-Level Performance on ImageNet Classification, Computer Vision Foundation, pg. 1026 - 1034\n",
        "- Yann LeCun, Léon Bottou, Genevieve B. Orr, and Klaus-Robert Müller. 1998. Efficient BackProp. In Neural Networks: Tricks of the Trade, this book is an outgrowth of a 1996 NIPS workshop. Springer-Verlag, Berlin, Heidelberg, 9–50.\n",
        "- Hendrycks, Dan and Kevin Gimpel. “Gaussian Error Linear Units (GELUs).” arXiv: Learning (2016): n. pag.\n",
        "- Team, K. (n.d.). Keras documentation: Keras layers API. https://keras.io/api/layers/\n",
        "- Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37 (ICML'15). JMLR.org, 448–456.\n",
        "- Team, K. (n.d.-a). Keras documentation: BatchNormalization layer. https://keras.io/api/layers/normalization_layers/batch_normalization/\n",
        "- \"API Reference.\" Tensorflow, https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n"
      ],
      "metadata": {
        "id": "2kr8Q-9T_nAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=5>⌛</font> <strong><font color=orange size=5>Do not exceed competition's runtime limit!</font></strong>\n",
        "\n",
        "<hr color=red>\n"
      ],
      "metadata": {
        "id": "DoF2GoB_QGw9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nosV1OWFJPx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a5bb124-adce-45db-a343-5e122821727b"
      },
      "source": [
        "tmr.ShowTime()    # measure Colab's runtime. Do not remove. Keep as the last cell in your notebook."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime is 292 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udpl5HJ4JSLr"
      },
      "source": [
        "# 💡**Starter Ideas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWllDRgiJVDJ"
      },
      "source": [
        "1. Learn about DNA [&#127910;](https://www.youtube.com/results?search_query=nucleotides+genes+amino+acids+)\n",
        "1. Try a larger training sample.\n",
        "1. Try longer training DNA strings, but SBERT may have a cap on string length, so you might split DNA into several strings and then concatenate or average resulting vectors\n",
        "1. Try other pretrained SBERT models. Note that DNA sequence uses ACGT letters, but many other models were trained on multilingual text. So, you might prefer those that were trained on mostly ASCII.\n",
        "1. SBERT is trained on word tokens (typically, separated by spaces), but DNA sequence has no spaces. Try placing spaces after every character or some semantically meaningful subsequences (this might require more domain knowledge).\n",
        "1. Try Google's [USE](https://tfhub.dev/google/universal-sentence-encoder-multilingual/3) embedding models\n",
        "1. Try Facebook's [LASER](https://github.com/facebookresearch/LASER) and [others](https://tfhub.dev/s?module-type=text-language-model).\n",
        "1. Try [Enformer](https://tfhub.dev/deepmind/enformer/1) for gene expressions. See [DeepMind paper](https://deepmind.com/blog/article/enformer).\n",
        "1. Try building your own embeddings on the given sequences. SBERT and other packages make it easy (just a few lines), but it may take too much time.\n",
        "1. Assess distribution of character patterns (single, doubles, triplets, ...). For example, an ACGT string generates AC, CG, GT doubles and ACG and CGT triplets. Does one class have more subsequences of some type? This might be a feature in your model.\n",
        "1. Try features built as counts of subsequences (singles, doubles, triplets, ...). Consider EDA first.\n",
        "1. Concatenate or otherwise combine multiple embeddings derived from each gene string\n",
        "1. Learn from [*The genetic code*](https://www.khanacademy.org/science/ap-biology/gene-expression-and-regulation/translation/a/the-genetic-code-discovery-and-properties), Khan Academy.\n",
        "1. Learn from [*Apply Machine Learning Algorithms for Genomics Data Classification*](https://medium.com/mlearning-ai/apply-machine-learning-algorithms-for-genomics-data-classification-132972933723)\n",
        "1. Learn from [*Efficient counting of k-mers in DNA sequences using a bloom filter*](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-333) Páll Melsted et al. 2011\n",
        "1. Try [Byte Pair Encoding](https://www.derczynski.com/papers/archive/BPE_Gage.pdf) and [SentencePiece](https://arxiv.org/pdf/1808.06226.pdf) to auto identification of \"important\" [k-mers](https://en.wikipedia.org/wiki/K-mer) (substrings)"
      ]
    }
  ]
}